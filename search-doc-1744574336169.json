{"searchDocs":[{"title":"Release 0.12.0","type":0,"sectionRef":"#","url":"/flowtide/blog/release-0-12-0","content":"","keywords":"","version":null},{"title":"Major changes​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#major-changes","content":" ","version":null,"tagName":"h2"},{"title":"All Processing Operators Updated to Column-Based Events​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#all-processing-operators-updated-to-column-based-events","content":" All processing operators now use the column-based event format, leading to better performance. However, some sources and sinks for connectors still use the row-based event format. Additionally, a few functions continue to rely on the row-based event format.  ","version":null,"tagName":"h3"},{"title":"MongoDB Source Support​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#mongodb-source-support","content":" This release adds support to read data from MongoDB, this includes using MongoDBs change stream to directly react on data changes.  ","version":null,"tagName":"h3"},{"title":"SQL Server Support for Stream State Persistence​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#sql-server-support-for-stream-state-persistence","content":" You can now store the stream state in SQL Server. For setup instructions, refer to the documentation:https://koralium.github.io/flowtide/docs/statepersistence#sql-server-storage  ","version":null,"tagName":"h3"},{"title":"Timestamp with Time Zone Data Type​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#timestamp-with-time-zone-data-type","content":" A new data type for timestamps has been added. This ensures that connectors can correctly use the appropriate data type, especially when writing. For example, writing to MongoDB now uses the BSON Date type.  ","version":null,"tagName":"h3"},{"title":"Minor Changes​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#minor-changes","content":" ","version":null,"tagName":"h2"},{"title":"Virtual Table Support​","type":1,"pageTitle":"Release 0.12.0","url":"/flowtide/blog/release-0-12-0#virtual-table-support","content":" Static data selection is now supported. Example usage:  INSERT INTO output SELECT * FROM ( VALUES (1, 'a'), (2, 'b'), (3, 'c') )  ","version":null,"tagName":"h3"},{"title":"Release 0.13.0","type":0,"sectionRef":"#","url":"/flowtide/blog/release-0-13-0","content":"","keywords":"","version":null},{"title":"Major changes​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#major-changes","content":" ","version":null,"tagName":"h2"},{"title":"New serializer to improve serialization speed​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#new-serializer-to-improve-serialization-speed","content":" A new custom serializer has been implemented that follows the Apache Arrow serialization while minimizing extra allocations and memory copies.  Additionally, the default compression method was also changed from using ZLib to Zstd. This change was also made to improve serialization performance.  ","version":null,"tagName":"h3"},{"title":"Support for pause & resume​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#support-for-pause--resume","content":" A new feature has been added to allow pausing and resuming data streams, making it easier to conduct maintenance or temporarily halt processing without losing state.  For more information, visit https://koralium.github.io/flowtide/docs/deployment/pauseresume.  ","version":null,"tagName":"h3"},{"title":"Integer column changed from 64 bits to dynamic size​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#integer-column-changed-from-64-bits-to-dynamic-size","content":" The integer column was changed to now instead select the bit size based on the data inside of the column. This change reduces memory usage for columns with smaller integer values. Bit size is determined on a per-page basis, so pages with larger values will only use higher bit sizes when necessary.  ","version":null,"tagName":"h3"},{"title":"Delta Lake Support​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#delta-lake-support","content":" This version adds support to both read and write to the Delta Lake format. This allows easy integration to data lake storage. To learn more about delta lake support, please visit: https://koralium.github.io/flowtide/docs/connectors/deltalake  ","version":null,"tagName":"h3"},{"title":"Custom data source & sink changed to use column based events​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#custom-data-source--sink-changed-to-use-column-based-events","content":" Both the custom data source and sink have now been changed to use column based events. This improves connector performance by eliminating the need to convert data between column-based and row-based formats during streaming.  ","version":null,"tagName":"h3"},{"title":"Minor changes​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#minor-changes","content":" ","version":null,"tagName":"h2"},{"title":"Elasticsearch connector change from Nest to Elastic.Clients.Elasticsearch​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#elasticsearch-connector-change-from-nest-to-elasticclientselasticsearch","content":" The Elasticsearch connector has been updated from the deprecated Nest package to Elastic.Clients.Elasticsearch. This change requires stream configurations to be adjusted for the new connection settings.  Additionally, connection settings are now provided via a function, enabling dynamic credential management, such as rolling passwords.  ","version":null,"tagName":"h3"},{"title":"Add support for custom stream listeners​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#add-support-for-custom-stream-listeners","content":" Applications can now listen to stream events like checkpoints, state changes, and failures, allowing for custom exit strategies or monitoring logic.  Example:  .AddCustomOptions(s =&gt; { s.WithExitProcessOnFailure(); });   ","version":null,"tagName":"h3"},{"title":"Cache lookup table for state clients​","type":1,"pageTitle":"Release 0.13.0","url":"/flowtide/blog/release-0-13-0#cache-lookup-table-for-state-clients","content":" An internal optimization adds a small lookup table for state client page access, reducing contention on the global LRU cache. This change has shown a 10–12% performance improvement in benchmarks. ","version":null,"tagName":"h3"},{"title":"Release 0.11.0","type":0,"sectionRef":"#","url":"/flowtide/blog/release-0-11-0","content":"","keywords":"","version":null},{"title":"Major Changes​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#major-changes","content":" ","version":null,"tagName":"h2"},{"title":"Column-Based Event Format​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#column-based-event-format","content":" Most operators have transitioned from treating events as rows with flexbuffer to a column-based format following the Apache Arrow specification. This change has led to significant performance improvements, especially in the merge join and aggregation operators. Transitioning from row-based to column-based events involved a major rewrite of core components, and some operators still use the row-based format, which will be updated in the future.  Not all expressions have been converted to work with column data yet. However, the solution currently handles conversions between formats to maintain backward compatibility. Frequent conversions may result in performance decreases.  The shift to a column format also introduced the use of unmanaged memory for new data structures, for the following reasons:  64-byte aligned memory addresses for optimal SIMD/AVX operations.Immediate memory return when a page is removed from the LRU cache, instead of waiting for the next garbage collection cycle.  With unmanaged memory, it is now possible to track memory allocation by different operators, providing better insight into memory usage in your stream.  ","version":null,"tagName":"h3"},{"title":"B+ Tree Splitting by Byte Size​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#b-tree-splitting-by-byte-size","content":" Previously, the B+ tree determined page sizes based on the number of elements, splitting pages into two equal parts when the max size (e.g., 128 elements) was reached. While this worked for streams with uniform element sizes, it led to size discrepancies in other cases, affecting serialization time and slowing down the stream.  This update introduces page splitting based on byte size, with a default page size of 32KB, ensuring more consistent and predictable page sizes.  ","version":null,"tagName":"h3"},{"title":"Initial SQL Type Validation​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#initial-sql-type-validation","content":" This release contains the beginning of type validation when creating a Substrait plan using SQL. Currently, only SQL Server provides specific type metadata, while sources like Kafka continue to designate columns as 'any' due to varying payload types.  The new validation feature raises exceptions for type mismatches, such as when a boolean column is compared to an integer (e.g., boolColumn = 1). This helps inform users transitioning from SQL Server that bit columns are treated as boolean in Flowtide.  ","version":null,"tagName":"h3"},{"title":"New UI​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#new-ui","content":"   A new UI has been developed, featuring an integrated time series database that enables developers to monitor stream behavior over time. This database’s API aligns with Prometheus standards, allowing for custom queries to investigate potential issues.  The UI retrieves all data through the Prometheus API endpoint, enabling future deployment as a standalone tool connected to a Prometheus server.  ","version":null,"tagName":"h3"},{"title":"Minor Changes​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#minor-changes","content":" ","version":null,"tagName":"h2"},{"title":"Congestion Control Based on Cache Misses​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#congestion-control-based-on-cache-misses","content":" Flowtide processes data in small batches, typically 1-100 events. While this approach works well with in-memory data, cache misses that require persistent storage access can create bottlenecks. This is particularly problematic with multiple chained joins, where sequential data fetching can delay processing.  To address this, the join operator now monitors cache misses during a batch and, when a threshold is reached, splits the processed events and forwards them to the next operator. This change allows operators to access persistent storage in parallel, easing congestion.  ","version":null,"tagName":"h3"},{"title":"Reduce the amount of pages written to persistent storage​","type":1,"pageTitle":"Release 0.11.0","url":"/flowtide/blog/release-0-11-0#reduce-the-amount-of-pages-written-to-persistent-storage","content":" Previously, all B+ tree metadata was written to persistent storage at every checkpoint, including root page IDs. In streams with numerous operators, this led to unnecessary writes.  Now, metadata is only written if changes have occurred, reducing the number of writes and improving storage efficiency. ","version":null,"tagName":"h3"},{"title":"Catalogs","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/catalogs","content":"Catalogs It is possible to add connectors under catalogs, this can be useful if one wants to read data from data sources that may have the same logical name. Say two SQL Servers that have the same database names (or even schema and table names). Example reading data from a catalog when using SQL Server: ... FROM {catalogName}.{databaseName}.{schema}.{tableName} A catalog is added on the connector manager as follows: connectorManager.AddCatalog(&quot;catalogName&quot;, (connectors) =&gt; { // Add connectors here as normal }); It works the same with dependency injection: services.AddFlowtideStream(&quot;my_stream_name&quot;) ... .AddConnectors(connectorManager =&gt; { connectorManager.AddCatalog(&quot;catalogName&quot;, (connectors) =&gt; { // Add connectors here as normal }); }); ","keywords":"","version":"Next"},{"title":"CosmosDB Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/cosmosdb","content":"","keywords":"","version":"Next"},{"title":"Sink​","type":1,"pageTitle":"CosmosDB Connector","url":"/flowtide/docs/connectors/cosmosdb#sink","content":" The CosmosDB sink allows the insertion of data into a CosmosDB container.  info All CosmosDB insertions must contain a column called 'id' and also a column that matches the configured partition key on the CosmosDB container.  Its implementation waits fully until the stream has reached a steady state at a time T until it writes data to the container. This means that its table output can always be traced back to a state from the source systems.  To use the CosmosDB Sink add the following line to the IConnectorManager:  connectorManager.AddCosmosDbSink(&quot;your regexp on table names&quot;, connectionString, databaseName, containerName);   ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"CosmosDB Connector","url":"/flowtide/docs/connectors/cosmosdb#example","content":" Having a column named 'id' and also a column matching the configured primary key is required for the sink to function.  sqlBuilder.Sql(@&quot; INSERT into cosmos SELECT userKey as id, companyId as pk, firstName, lastName FROM users &quot;); connectorManager.AddCosmosDbSink(&quot;cosmos&quot;, connectionString, databaseName, containerName); ...  ","version":"Next","tagName":"h3"},{"title":"Elasticsearch Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/elasticsearch","content":"","keywords":"","version":"Next"},{"title":"Sink​","type":1,"pageTitle":"Elasticsearch Connector","url":"/flowtide/docs/connectors/elasticsearch#sink","content":" The ElasticSearch sink allows insertion into an index.  info All ElasticSearch insertions must contain a column called '_id' this column is the unique identifier in the elasticsearch index. This field will not be added to the source fields.  To use the ElasticSearch Sink add the following line to the ConnectorManager:  connectorManager.AddElasticsearchSink(&quot;*&quot;, new FlowtideElasticsearchOptions() { ConnectionSettings = () =&gt; new ElasticsearchClientSettings(new Uri(...)) });   The table name in the write relation becomes the index the sink writes to. The connection settings are a function to allow the usage of rolling passwords when connecting to elasticsearch.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Elasticsearch Connector","url":"/flowtide/docs/connectors/elasticsearch#example","content":" Having a column named '_id' is required for the sink to function.  sqlBuilder.Sql(@&quot; INSERT into elastic_index_name SELECT userKey as _id, userKey, companyId, firstName, lastName FROM users &quot;); connectorManager.AddElasticsearchSink(&quot;*&quot;, new FlowtideElasticsearchOptions() { ConnectionSettings = () =&gt; new ElasticsearchClientSettings(new Uri(...)) }); ...   ","version":"Next","tagName":"h3"},{"title":"Set alias on initial data completion​","type":1,"pageTitle":"Elasticsearch Connector","url":"/flowtide/docs/connectors/elasticsearch#set-alias-on-initial-data-completion","content":" One way to integrate with elasticsearch is to create a new index for each new stream version and change an alias to point to the new index. This is possible by using the GetIndexNameFunc and OnInitialDataSent functions in the options.  Example:  connectorManager.AddElasticsearchSink(&quot;*&quot;, new FlowtideElasticsearchOptions() { ConnectionSettings = () =&gt; connectionSettings, CustomMappings = (props) =&gt; { // Add cusotm mappings }, GetIndexNameFunc = (writeRelation) =&gt; { // Set an index name that will be unique for this run // The index name must be possible to be recovered between crashes to write to the same index return $&quot;{writeRelation.NamedObject.DotSeperated}-{tagVersion}&quot;; }, OnInitialDataSent = async (client, writeRelation, indexName) =&gt; { var aliasName = writeRel.NamedObject.DotSeperated; var getAliasResponse = await client.Indices.GetAliasAsync(new Elastic.Clients.Elasticsearch.IndexManagement.GetAliasRequest(name: aliasName)); var putAliasResponse = await client.Indices.PutAliasAsync(indexName, writeRel.NamedObject.DotSeperated); var oldIndices = getAliasResponse.Aliases.Keys.ToList(); if (putAliasResponse.IsSuccess()) { foreach (var oldIndex in oldIndices) { if (oldIndex != indexName) { await client.Indices.DeleteAsync(oldIndex); } } } else { throw new InvalidOperationException(putAliasResponse.ElasticsearchServerError!.Error.StackTrace); } }, });  ","version":"Next","tagName":"h3"},{"title":"Delta Lake Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/deltalake","content":"","keywords":"","version":"Next"},{"title":"Options​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#options","content":" These are the options that can be configured when adding the delta lake connector:  Name\tDescription\tDefault\tRequiredStorageLocation\tConnection where the tables are located\tNull\tYes OneVersionPerCheckpoint\tOnly used for reading, it then makes sure that each commit is sent once per checkpoint\tFalse\tNo DeltaCheckInterval\tOnly used for reading, how often it should be checked if a new commit exists\t10 seconds\tNo WriteChangeDataOnNewTables\tIf new tables created by the connector should enable change feed\tFalse\tNo EnableDeletionVectorsOnNewTables\tIf new tables created by the connector should enable deletion vectors\tTrue\tNo  ","version":"Next","tagName":"h2"},{"title":"Delta Lake Source​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#delta-lake-source","content":" The delta lake source allows ingesting data from a delta lake table. To connect to a delta lake table, use AddDeltaLake method on the connector manager.  connectorManager.AddDeltaLakeSource(new DeltaLakeOptions() { // Connect to local disk, azure, AWS etc here, add the directory beneath the actual table you want to query (table name and folders are selected in the query) StorageLocation = Files.Of.LocalDisk(&quot;./testdata&quot;) });   The delta lake connector uses Stowage to allow connection to different cloud storage solutions, please visit that link to check possible connections.  Important to note is that the directory of storage location should be beneath the actual table you want to query. The actual table is selected in the query. Example:  INSERT INTO output SELECT * FROM my_delta_lake_table   In combination with the connector manager addition above, this will use the path ./testdata/my_delta_lake_table.  If you instead would write:  INSERT INTO output SELECT * FROM my_folder.my_delta_lake_table   It becomes: ./testdata/my_fylder/mmy_delta_lake_table.  The delta lake source can calculate all the changes from the table, so no additional state is stored in the stream of the data to correctly calculate changes.  Supported features  Calculate change data from add/remove actionsUse cdc files if they exist for change dataDeletion vectorsPartitioned dataColumn mapping  ","version":"Next","tagName":"h2"},{"title":"Replaying delta changes​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#replaying-delta-changes","content":" One feature of the delta lake source is the possibility to replay the log where each commit to the delta table is sent separately once per checkpoint. This can be useful in unit tests, but also if one wants to send historic data to a destination system such as a time-series database and not skip over a commit.  To use the replay functionality use the following setting in options:  connectorManager.AddDeltaLakeSource(new DeltaLakeOptions() { StorageLocation = Files.Of.LocalDisk(&quot;./testdata&quot;), OneVersionPerCheckpoint = true });   ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#sample","content":" There is an example in the samples folder that uses the Delta Lake Source to read data from an azure blob storage. To run the example, start the AspireSamples project and select one of:  DeltaLake-SourceDeltaLake-Source, Replay history  After the project has started, inspect the console log of the application to see a log output of the rows.  ","version":"Next","tagName":"h3"},{"title":"Azure Blob Storage Configuration Example​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#azure-blob-storage-configuration-example","content":" When using Azure Blob Storage you would configure the storage example like this example:  connectors.AddDeltaLakeSource(new DeltaLakeOptions() { StorageLocation = Files.Of.AzureBlobStorage(accountName, accountKey) });   This connects the source to the root of the blob storage, to then query a table you must include the container name and all subfolders to the table location:  SELECT * FROM my_container.my_optional_parent_folder.my_table   ","version":"Next","tagName":"h3"},{"title":"Delta Lake Sink​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#delta-lake-sink","content":" The delta lake sink allows to materialize/denormalize queries as a delta lake table. The connector will continuously update the table on each checkpoint. It does not write on each watermark update, this is done to reduce the amount of files that are written and try to only write consistent updates. This means that the write frequency can be modified by changing how often a checkpoint is taken.  To add the delta lake sink, write the following to the connector manager:  connectors.AddDeltaLakeSink(new DeltaLakeOptions() { StorageLocation = Files.Of.LocalDisk(&quot;./testdata&quot;) });   Delta Lake Features:  Deletion vectorsStatistics outputData file skippingChange data files  warning The delta lake sink does not yet support partitioned tables.  ","version":"Next","tagName":"h2"},{"title":"Change data feed​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#change-data-feed","content":" It is possible to enable flowtide to write change data files which can help speed up change data readers. This is done by setting the flag WriteChangeDataOnNewTables to true in the options when configuring the delta lake connector. If it is an existing table, the writerFeatures in protocol must contain changeDataFeed and the metadata configuration must contain delta.enableChangeDataFeedset to true.  This creates new files in a _change_data folder and adds the corresponding actions in the delta log. Change data files are only added if there are deletes on existing rows, since the add action gives the same performance to read change data if there are no deletes. This also saves on storage space.  To be able to reduce memory usage, Flowtide writes the change data files as it creates a commit, but they can then be deleted before completing the commit.  Since this connector writes the exact result set in the query, it does not have the concept of primary keys and rows can be duplicated if that is the result of the query. This means that the change data feed only contains delete and insert operations.  ","version":"Next","tagName":"h3"},{"title":"Supported data types​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#supported-data-types","content":" Flowtide can write the following data types to a delta lake table:  ArrayBinaryBooleanDateDecimalFloatDoubleByteShortIntegerLongStringTimestampStructMap  ","version":"Next","tagName":"h3"},{"title":"Creating a new table​","type":1,"pageTitle":"Delta Lake Connector","url":"/flowtide/docs/connectors/deltalake#creating-a-new-table","content":" It is possible to create a new delta lake table if it does not exist. This is done by using the CREATE TABLE syntax to clearly set the data types for each column.  Not all delta lake data types are supported in Flowtide SQL, so for full type support please create an empty table manually. The data types that are supported with CREATE TABLE and how they are mapped are:  Flowtide SQL name\tFlowtide Internal Type\tDelta Lake name\tCommentint\tInteger\tlong\tIn flowtide integers are treated as the same type, the size is dynamically increased. binary\tbinary\tbinary boolean\tboolean\tboolean date\ttimestamp\tdate decimal\tdecimal\tdecimal\tFlowtide uses C# decimal format, this then gets converted into a decimal with precision and scale. double\tdouble\tdouble\tFlowtide only manages double precision floating point numbers. timestamp\ttimestamp\ttimestamp struct\tmap\tstruct\tMap should be used to create struct values at this point. map\tmap\tmap list\tlist\tarray\t  Sql example​  CREATE TABLE test ( intval INT, binval BINARY, boolval BOOLEAN, dateval DATE, decimalVal DECIMAL(19, 3), doubleVal DOUBLE, timeVal TIMESTAMP, structVal STRUCT&lt;firstName STRING, lastName STRING&gt;, mapVal MAP&lt;STRING, STRING&gt;, listVal LIST&lt;STRING&gt; ); INSERT INTO test SELECT intval, binval, boolval, dateval, decimalVal, doubleVal, timeVal, MAP('firstName', firstName, 'lastName', lastName) as structVal, MAP('firstName', firstName, 'lastName', lastName) as mapVal, list(firstName, lastName) as listVal FROM my_source_table;  ","version":"Next","tagName":"h3"},{"title":"Configuration Data","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/configurationdata","content":"","keywords":"","version":"Next"},{"title":"Options Data Source​","type":1,"pageTitle":"Configuration Data","url":"/flowtide/docs/connectors/configurationdata#options-data-source","content":" The options data source uses IOptionsMonitor&lt;TOptions&gt; to listen to changes and to get the configuration settings.  The Options data source is built-in as a default connector when installing FlowtideDotNet, so no extra nuget package is required. Options are added with the AddOptionsSource&lt;TOptions&gt; method on the connector manager. One of the main benefits of this connector comes if you have a configuration provider that allows reloading, then data can be updated without restarting the stream.  Example:  internal class TestOptions { public string? Name { get; set; } } var builder = WebApplication.CreateBuilder(args); builder.Services.AddOptions&lt;TestConfig&gt;() .Bind(builder.Configuration.GetSection(&quot;config&quot;)); builder.Services.AddFlowtideStream(&quot;test&quot;) .AddConnectors((connectorManager) =&gt; { connectorManager.AddOptionsSource&lt;TestConfig&gt;(&quot;config&quot;); ... }) ...   Configuration settings:  { &quot;config&quot;: { &quot;name&quot;: &quot;hello&quot; } }   SQL statement:  INSERT INTO output SELECT name FROM config   This stream would result in the following data:  namehello  ","version":"Next","tagName":"h2"},{"title":"Example using options data source as an exclude filter​","type":1,"pageTitle":"Configuration Data","url":"/flowtide/docs/connectors/configurationdata#example-using-options-data-source-as-an-exclude-filter","content":" One scenario where this might be useful is to handle exclusions, say a specific row should not be sent to a destination.  You could then have an options class similar to:  internal class TestOptions { public List&lt;string&gt;? ExcludedIds { get; set; } }   And an sql that looks the following:  CREATE VIEW excluded_ids WITH (BUFFERED = true) AS select excludedId FROM config c INNER JOIN UNNEST(c.excludedIds) excludedId; INSERT INTO my_destination SELECT m.id, m.other FROM my_table m LEFT JOIN excluded_ids e ON m.id = e.excludedId WHERE e.excludedId is null;   The buffered view helps with performance where it buffers the changes from the UNNEST statement so only changing rows are returned. We can then do a left join to match with the excluded ids but only return rows where there was no match. ","version":"Next","tagName":"h3"},{"title":"Custom Data Source","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/customdata","content":"","keywords":"","version":"Next"},{"title":"Generic Data Source​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#generic-data-source","content":" The generic data source allows easy implementation against custom sources that returns C# objects.  It allows:  Full batch reloads, where all the data is imported again and delta is computed.Delta loads, where delta should be returned.Custom watermark provided by the source.Scheduling of full batch and delta reloads.  There are two classes that can be implemented for the generic data source:  GenericDataSourceAsync - Data is returned by an IAsyncEnumerable, this should be used with remote sources.GenericDataSource - Data is returned by an IEnumerable, which should be used in cases where data is already in memory.  When implementing a generic data source, it is important to think about memory usage, for instance, do not fetch all rows and store them in memory and then return them, this can cause huge memory spikes or out of memory. Instead yield return values and fetch the data in batches. The operator stores the data in B+ trees that will be temporarily stored on disk if the memory usage is too high.  ","version":"Next","tagName":"h2"},{"title":"Implementation example​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#implementation-example","content":" public class ExampleDataSource : GenericDataSourceAsync&lt;User&gt; { private readonly IUserRepository _userRepository; public ExampleDataSource(IUserRepository userRepository) { _userRepository = userRepository; } // Fetch delta every 1 second public override TimeSpan? DeltaLoadInterval =&gt; TimeSpan.FromSeconds(1); // Reload all data every 1 hours, this is not required, but can be useful. // If for instance deletes cant be found in deltas from the source, // a full reload would find all deleted rows. public override TimeSpan? FullLoadInterval =&gt; TimeSpan.FromHours(1); protected override IEnumerable&lt;FlowtideGenericObject&lt;User&gt;&gt; DeltaLoadAsync(long lastWatermark) { var changes = _userRepository.GetChangesFromWatermarkAsync(lastWatermark); await foreach(var change in changes) { yield return new FlowtideGenericObject&lt;User&gt;(change.Id, change, change.Timestamp); } } protected override IEnumerable&lt;FlowtideGenericObject&lt;User&gt;&gt; FullLoadAsync() { var data = _userRepository.GetAllDataAsync(lastWatermark); await foreach(var row in data) { yield return new FlowtideGenericObject&lt;User&gt;(row.Id, row, row.Timestamp); } } }   To use your data source, add the following to the ConnectorManager:  connectorManager.AddCustomSource( &quot;{the table name}&quot;, (readRelation) =&gt; new ExampleDataSource(userRepository));   All rows must return an identifier that is used when calculating changes of the data. This key can be accessed when querying with the special name of __key.  ","version":"Next","tagName":"h3"},{"title":"Trigger data reloads programatically​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#trigger-data-reloads-programatically","content":" The generic data source also registers triggers that allows the user to notify the stream when a reload should happen.  The following triggers are registered:  full_load - Does a full load on all running generic data sourcesdelta_load - Does a delta load on all running generic data sourcesfull_load_{tableName} - Full load for a specific sourcedelta_load_{tableName} - Delta load for a specific source  Example on calling a trigger:  await stream.CallTrigger(&quot;delta_load&quot;, default);   Calling the triggers programatically can be useful if having an interval would cause too much latency for the data.  ","version":"Next","tagName":"h3"},{"title":"SQL Table Provider​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#sql-table-provider","content":" There is a table provider as well for the generic data source, that can be used to easily import table metadata from a class.  Example:  sqlPlanBuilder.AddGenericDataTable&lt;User&gt;(&quot;users&quot;);   If you are starting Flowtide with dependency injection, a table provider is added automatically, so this step is not required.  ","version":"Next","tagName":"h3"},{"title":"Generic data sink​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#generic-data-sink","content":" The generic data sink allow the implementation of a sink that reads the rows as C# classes. This limits the stream to only send those specific columns, but it can be useful im cases such as API integrations where there is a strict schema.  ","version":"Next","tagName":"h2"},{"title":"Implementation example​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#implementation-example-1","content":" Create a class that inherits from GenericDataSink.  internal class TestDataSink : GenericDataSink&lt;User&gt; { public override Task&lt;List&lt;string&gt;&gt; GetPrimaryKeyNames() { return Task.FromResult(new List&lt;string&gt; { &quot;{primaryKeyColumnName}&quot; }); } public override async Task OnChanges(IAsyncEnumerable&lt;FlowtideGenericWriteObject&lt;User&gt;&gt; changes, Watermark watermark, bool isInitialData, CancellationToken cancellationToken) { await foreach(var userChange in changes) { if (!userChange.IsDeleted) { // Do upsert to destination } else { // Do delete against destination } } } }   Add the generic data sink to the ConnectorManager:  connectorManager.AddCustomSink(&quot;{tableName}&quot;, (rel) =&gt; new testDataSink());   ","version":"Next","tagName":"h3"},{"title":"Adding custom converters​","type":1,"pageTitle":"Custom Data Source","url":"/flowtide/docs/connectors/customdata#adding-custom-converters","content":" It is possible to register custom converters to convert .NET objects into the column format, and back. This is done by overriding the GetCustomConverters method in both generic sources and sinks.  One example is for instance to use strings for enums instead of integers. Example:  internal class MyDataSource : GenericDataSource&lt;MyClass&gt; { ... public override IEnumerable&lt;IObjectColumnResolver&gt; GetCustomConverters() { // Returns a new converter resolver for enums that will use strings instead of integers yield return new EnumResolver(enumAsStrings: true); } ... }  ","version":"Next","tagName":"h2"},{"title":"Kafka Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/kafka","content":"","keywords":"","version":"Next"},{"title":"Source​","type":1,"pageTitle":"Kafka Connector","url":"/flowtide/docs/connectors/kafka#source","content":" The Kafka Source allows a stream to fetch data from a kafka stream as a table in a stream.  The source is added to the ConnectorManager with the following line:  connectorManager.AddKafkaSource(&quot;your regexp on table names&quot;, new FlowtideKafkaSourceOptions() { ConsumerConfig = kafkaConsumerConfig, KeyDeserializer = keyDeserializer, ValueDeserializer = valueDeserializer });   The table name in the read relation becomes the topic name the source will read from.  The following key deserializers exist:  FlowtideKafkaJsonKeyDeserializer - deserializes a json keyFlowtidekafkaStringKeyDeserializer - deserializes a string key  The following value deserializers exist:  FlowtideKafkaUpsertJsonDeserializer - Deserializes the value as json, if the value is not null, its an upsert, if it is null, its a delete.  The source has a special column name for the key, it is called _key which can be used to select the key value from a kafka message.  ","version":"Next","tagName":"h2"},{"title":"Usage in SQL​","type":1,"pageTitle":"Kafka Connector","url":"/flowtide/docs/connectors/kafka#usage-in-sql","content":" CREATE TABLE my_kafka_topic ( _key, firstName, lastName ); INSERT INTO outputtable SELECT _key, firstName, lastName FROM my_kafka_topic;   ","version":"Next","tagName":"h3"},{"title":"Sink​","type":1,"pageTitle":"Kafka Connector","url":"/flowtide/docs/connectors/kafka#sink","content":" The kafka sink allows a stream to write events to kafka.  The sink is added to the ConnectorManager with the following line:  connectorManager.AddKafkaSink(&quot;your regexp on table names&quot;, new FlowtideKafkaSinkOptions() { KeySerializer = new FlowtideKafkaStringKeySerializer(), ProducerConfig = config, ValueSerializer = new FlowtideKafkaUpsertJsonSerializer() });   Same as the source, it writes to the topic name that is entered in the write relation.  Available key serializers:  FlowtideKafkaJsonKeySerializer - JSON serializes the key valueFlowtideKafkaStringKeySerializer - Outputs a string value, only works if the key is in a string type.  Available value serializers:  FlowtideKafkaUpsertJsonSerializer - Outputs the value as json, if it is a delete, it outputs null as the value.  ","version":"Next","tagName":"h2"},{"title":"Compare against existing data in Kafka​","type":1,"pageTitle":"Kafka Connector","url":"/flowtide/docs/connectors/kafka#compare-against-existing-data-in-kafka","content":" It is possible to fetch all existing data in Kafka when a new stream is starting. This data will be used on the initial result set to compare against to see if the data is already in kafka (the lateset message on a key).  It will also send delete operations for any keys that no longer exist in the result set.  To use this feature you must set the following properties:  FetchExistingConfig - Consumer config that will be used when fetching existing data.FetchExistingValueDeserializer - Deserializer that will be used, when deserializing existing data.FetchExistingKeyDeserializer - Deserializer for the key field in a kafka message.  ","version":"Next","tagName":"h3"},{"title":"Extend event processing logic​","type":1,"pageTitle":"Kafka Connector","url":"/flowtide/docs/connectors/kafka#extend-event-processing-logic","content":" There are two properties in the options that can help add extra logic to the kafka sink.  EventProcessor - Called on all events that will be sent to kafka.OnInitialDataSent - Called once on a new stream when the first data has been sent (usually after the first checkpoint).  These functions can help implement logic such as having an external state store for a kafka topic that keeps track of all the data that has been sent to the topic. If a stream is modified and republished and has to start from the beginning, this state store can then make sure only delta valeus are sent to the kafka topic, and deletions of events that no longer exists.  Example:  connectorManager.AddKafkaSink(&quot;your regexp on table names&quot;, new FlowtideKafkaSinkOptions() { ... EventProcessor = async (events) =&gt; { for (int i = 0; i &lt; events.Count; i++) { // Check if the event exists with the exact key and value in the store var exists = await dbLookup.ExistAsync(events[i].Key, events[i].Value); if (exists) { events.RemoveAt(i); i--; } // Add the data with a run version that should be unique of this stream version. await dbLookup.Add(events[i].Key, events[i].Value, runVersion); } }, OnInitialDataSent = (producer, writeRelation, topicName) =&gt; { // Get all events that does not have the latest run version await foreach (var e in dbLookup.GetEventsNotMatchingVersion(runVersion)) { // Send delete on all events that no longer exist await producer.SendAsync(new Message&lt;byte[], byte[]?&gt;() { Key = e.Key, Value = null }); } // Delete the events from the state store. await dbLookup.DeleteNotMatchingVersion(runVersion); } });  ","version":"Next","tagName":"h3"},{"title":"Permify Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/permify","content":"","keywords":"","version":"Next"},{"title":"Relationship Sink​","type":1,"pageTitle":"Permify Connector","url":"/flowtide/docs/connectors/permify#relationship-sink","content":" The relationship sink allows inserting relationship data from other sources into Permify.  These columns are required to insert data:  subject_type - subject typesubject_id - identifier of the subjectrelation - relation nameentity_type - Entity typeentity_id - identifier of the entity.  Optional:  subject_relation - optional subject relation.  To use the Permify Relationship Sink add the following line to the ConnectorManager:  connectorManager.AddPermifyRelationshipSink(&quot;regex pattern for tablename&quot;, new PermifySinkOptions() { Channel = grpcChannel, TenantId = tenantId });   The regex pattern should match what you write in SQL as the table name in the insert statement.  Sql example:  INSERT INTO permify SELECT 'user' as subject_type, o.userkey as subject_id, 'reader' as relation, 'document' as entity_type, o.orderkey as entity_id FROM orders o   ","version":"Next","tagName":"h2"},{"title":"Events​","type":1,"pageTitle":"Permify Connector","url":"/flowtide/docs/connectors/permify#events","content":" The following event listener exist so far in the Permify relationship sink:  OnWatermarkFunc - Called after a watermark is recieved and the data has been added to Permify, also contains the last recieved snaptoken from Permify.  ","version":"Next","tagName":"h3"},{"title":"Relationship Source​","type":1,"pageTitle":"Permify Connector","url":"/flowtide/docs/connectors/permify#relationship-source","content":" The relationship source allows reading relationship data from Permify and use it in your data streams.  The following columns are returned:  subject_type - subject typesubject_id - identifier of the subjectrelation - relation nameentity_type - resource typeentity_id - identifier of the resource.subject_relation - optional subject relation.  Example on using the relationship source:  connectorManager.AddPermifyRelationshipSource(&quot;regex pattern for tablename&quot;, new PermifySourceOptions() { Channel = grpcChannel, TenantId = tenantId });   ","version":"Next","tagName":"h2"},{"title":"Materialize/Denormalize Permissions​","type":1,"pageTitle":"Permify Connector","url":"/flowtide/docs/connectors/permify#materializedenormalize-permissions","content":" It is not yet possible to materialize/denormalize permissions from Permify. ","version":"Next","tagName":"h2"},{"title":"MongoDB Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/mongodb","content":"","keywords":"","version":"Next"},{"title":"Source​","type":1,"pageTitle":"MongoDB Connector","url":"/flowtide/docs/connectors/mongodb#source","content":" The MongoDB source allows a user to create a stream that reads data from MongoDB and any changes on the collection. The connector requires that change stream is enabled on the MongoDB instance to function.  To add the MongoDB source add the following line to the ConnectorManager:  connectorManager.AddMongoDbSource(&quot;connectionString&quot;);   To select data from MongoDB use {database}.{collectionName}.  You can also add MongoDB under a catalog:  connectorManager.AddCatalog(&quot;mymongodb&quot;, c =&gt; { c.AddMongoDbSource(&quot;connectionString&quot;); });   This will then be referenced with mymongodb.{database}.{collectionName}.  ","version":"Next","tagName":"h2"},{"title":"Accessing properties​","type":1,"pageTitle":"MongoDB Connector","url":"/flowtide/docs/connectors/mongodb#accessing-properties","content":" By default, the mongoDB source contains two properties defined:  _id - the _id field in MongoDB._doc - Map with key values of all the properties in the document.  Example usage:  SELECT _doc.firstName, _doc.lastName FROM {database}.{collection}   It is possible to define a schema for easier access by using CREATE TABLE:  CREATE TABLE {database}.{collection} ( firstName, lastName ); SELECT firstName, lastName FROM {database}.{collection}   Using create table limits the data that is sent through the stream and only sends out the properties defined in the create table.  warning When using _doc field the entire document is sent through the stream which can result in a performance decrease. A better alternative if one does not want to use CREATE TABLE is to use CREATE VIEW to project the map values earlier in the stream. CREATE VIEW mymongodb AS SELECT _doc.firstName, _doc.lastName FROM {database}.{collection}; SELECT firstName, lastName FROM mymongodb;   ","version":"Next","tagName":"h3"},{"title":"Sink​","type":1,"pageTitle":"MongoDB Connector","url":"/flowtide/docs/connectors/mongodb#sink","content":" The MongoDB sink allows the insertion of data into a MongoDB collection.  Its implementation waits fully until the stream has reached a steady state at a time T until it writes data to the collection. This means that its table output can always be traced back to a state from the source systems.  To use the MongoDB Sink add the following line to the ConnectorManager:  connectorManager.AddMongoDbSink(&quot;regex pattern for tablename&quot;, new FlowtideMongoDBSinkOptions() { Collection = collection, //MongoDB collection Database = databaseName, // MongoDB database ConnectionString = connectionString, //Connection string to MongoDB PrimaryKeys = primaryKeys //List of columns that will be treated as primary keys in the collection });   ","version":"Next","tagName":"h2"},{"title":"Overwriting data in a collection and cleaning up old data​","type":1,"pageTitle":"MongoDB Connector","url":"/flowtide/docs/connectors/mongodb#overwriting-data-in-a-collection-and-cleaning-up-old-data","content":" It is possible with the MongoDB sink to append metadata to documents and remove data from previous runs. This can be helpful when the stream is changed and you want to write to the same collection, but remove data from a previous run.  To do this we add the following code:  connectorManager.AddMongoDbSink(&quot;regex pattern for tablename&quot;, new FlowtideMongoDBSinkOptions() { ... TransformDocument = (doc) =&gt; { // version should come from configuration doc.Add(&quot;_metadata&quot;, run_version); }, OnInitialDataSent = async (collection) =&gt; { await collection.DeleteManyAsync(Builders&lt;BsonDocument&gt;.Filter.Not(Builders&lt;BsonDocument&gt;.Filter.Eq(&quot;_metadata&quot;, run_version))); } });   This will append a metadata field to all documents with the current run version. When the initial data from the stream has been saved, it will delete all documents that does not have the metadata information.  ","version":"Next","tagName":"h3"},{"title":"Watermark updates​","type":1,"pageTitle":"MongoDB Connector","url":"/flowtide/docs/connectors/mongodb#watermark-updates","content":" It is possible to listen to watermark updates, this is done by setting the OnWatermarkUpdate property in the options.  Example:  connectorManager.AddMongoDbSink(&quot;regex pattern for tablename&quot;, new FlowtideMongoDBSinkOptions() { ... OnWatermarkUpdate = async (watermark) =&gt; { // Inform other systems for instance about the watermark change. } });  ","version":"Next","tagName":"h3"},{"title":"Sharepoint Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/sharepoint","content":"","keywords":"","version":"Next"},{"title":"List Sink​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#list-sink","content":" The List Sink saves result from a stream into a sharepoint list.  To use it, add it to the ConnectorManager:  connectorManager.AddSharepointListSink(&quot;{regexPattern}&quot;, (writeRelation) =&gt; new SharepointSinkOptions() { // Set the primary key in sharepoint, it is the name of the column in sharepoint PrimaryKeyColumnNames = new List&lt;string&gt; { &quot;primaryKeyColumnName&quot; } SharepointUrl = &quot;{your company name}.sharepoint.com&quot;, Site = &quot;{Name of sharepoint site}&quot; TokenCredential = yourAzureTokenCredential })   ","version":"Next","tagName":"h2"},{"title":"Supported column types​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#supported-column-types","content":" The following column types are supported:  Text - Data in should be of data type stringPersonOrGroup - Data in should be of type string and contain a UPN such as email.Boolean - Data type should be a boolean (true/false).DateTime - Data in should be a unix timestamp.Multi line - Same as text.Choice - Data in should be of data type string and contain one of the choices.Number - Data in should be an integer or a float.Currency - Data in should be an integer or a float.  ","version":"Next","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#options","content":" The following options can be set on the sink:  PrimaryKeyColumnNames - Required, a list of sharepoint column names that make up the primary key.SharepointUrl - Required, url to the sharepoint tenant, should not contain 'https://'.Site - Required, name to the site where the list is.TokenCredential - Required, the token credential that is used to authenticate against sharepoint.DisableDelete - Disables row deletion.ThrowOnPersonOrGroupNotFound - Instead of returning null when a person is not found, throw an exception.PreprocessRow - Action that is called on each upsert, allows addition of metadata columns.  ","version":"Next","tagName":"h3"},{"title":"List Source​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#list-source","content":" The List Source allow a stream to read data and changes from a sharepoint list.  The List Source is added to the ConnectorManager as follows:  connectorManager.AddSharepointSource(new SharepointSourceOptions() { SharepointUrl = &quot;{your company name}.sharepoint.com&quot;, Site = &quot;{Name of sharepoint site}&quot;, TokenCredential = yourAzureTokenCredential }, &quot;{optional prefix}&quot;);   The following column types are supported:  TextLookupPersonOrGroupBooleanDateTime  More types will be added in the future. Please create a github issue for the type you are missing.  ","version":"Next","tagName":"h2"},{"title":"Special columns​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#special-columns","content":" There are some special column names that allows you to fetch data:  ID - The sharepoint row identifier._fields - Contains all columns in a map. Useful if one wants the stream to dynamically handle the addition of new columns  The data in the _fields column has the following structure:  { &quot;{columnName1}&quot;: { &quot;value&quot;: &quot;{columnValue}&quot;, &quot;description&quot;: &quot;{column description}&quot;, &quot;type&quot;: &quot;{columnType}&quot; }, &quot;{columnName2}&quot;: { ... } }   ","version":"Next","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"Sharepoint Connector","url":"/flowtide/docs/connectors/sharepoint#options-1","content":" The following options can be set on the source:  SharepointUrl - Required, url to the sharepoint tenant, should not contain 'https://'.Site - Required, name to the site where the list is.TokenCredential - Required, the token credential that is used to authenticate against sharepoint. ","version":"Next","tagName":"h3"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/flowtide/docs/deployment","content":"Deployment This section covers information that can be useful when deploying flowtide. 📄️ Pause And Resume It is possible to pause and resume a flowtide stream, this is useful when there might be maintainance work on a source system, or as a panic button to stop","keywords":"","version":"Next"},{"title":"AuthZed/SpiceDB Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/spicedb","content":"","keywords":"","version":"Next"},{"title":"Sink​","type":1,"pageTitle":"AuthZed/SpiceDB Connector","url":"/flowtide/docs/connectors/spicedb#sink","content":" The sink allows inserting data from other sources into SpiceDB.  These columns are required to insert data:  subject_type - subject typesubject_id - identifier of the subjectrelation - relation nameresource_type - resource typeresource_id - identifier of the resource.  Optional:  subject_relation - optional subject relation.  To use the SpiceDB Sink add the following line to the ConnectorManager:  connectorManager.AddSpiceDbSink(&quot;regex pattern for tablename&quot;, new SpiceDbSinkOptions { Channel = grpcChannel, // Grpc channel used to connect to SpiceDB GetMetadata = () =&gt; { var metadata = new Metadata(); // Add any headers etc here. metadata.Add(&quot;Authorization&quot;, &quot;Bearer {token}&quot;); return metadata; } });   Sql example:  INSERT INTO spicedb SELECT 'user' as subject_type, o.userkey as subject_id, 'reader' as relation, 'document' as resource_type, o.orderkey as resource_id FROM orders o   ","version":"Next","tagName":"h2"},{"title":"Events​","type":1,"pageTitle":"AuthZed/SpiceDB Connector","url":"/flowtide/docs/connectors/spicedb#events","content":" The following event listeners exist that can be used to modify or get the current watermark of the stream that has been sent to SpiceDB:  BeforeWriteRequestFunc - Called before each write, its possible to modify the data before it gets sent here.OnWatermarkFunc - Called after a watermark is recieved and the data has been added to SpiceDB, also contains the last recieved zedtoken from SpiceDB.OnInitialDataSentFunc - Called the first time data has been written to SpiceDB.  ","version":"Next","tagName":"h3"},{"title":"Delete existing data if not updated​","type":1,"pageTitle":"AuthZed/SpiceDB Connector","url":"/flowtide/docs/connectors/spicedb#delete-existing-data-if-not-updated","content":" It is possible to delete existing data in SpiceDB if it is not in the result set of the stream. This is done by passing in the property DeleteExistingDataFilter which is the filter of what data to fetch. If your stream say updates resource type document and relation reader you should pass that in as the filter if you wish to delete existing data that is not from the current stream.  This will cause all data to be downloaded into the stream which will cause a slower performance to read the initial data.  ","version":"Next","tagName":"h3"},{"title":"Source​","type":1,"pageTitle":"AuthZed/SpiceDB Connector","url":"/flowtide/docs/connectors/spicedb#source","content":" The source allows reading data from SpiceDB. The following columns are returned:  subject_type - subject typesubject_id - identifier of the subjectrelation - relation nameresource_type - resource typeresource_id - identifier of the resource.subject_relation - optional subject relation.  Filter conditions on resource type, relation and subject type will tried to be pushed down in the query to SpiceDB if possible.  Example on using the spicedb source:  connectorManager.AddSpiceDbSource(&quot;regex pattern for tablename&quot;, new SpiceDbSourceOptions { Channel = grpcChannel, GetMetadata = () =&gt; { var metadata = new Metadata(); // Add any headers etc here. metadata.Add(&quot;Authorization&quot;, &quot;Bearer {token}&quot;); return metadata; } });   ","version":"Next","tagName":"h2"},{"title":"Materialize/Denormalize Permissions​","type":1,"pageTitle":"AuthZed/SpiceDB Connector","url":"/flowtide/docs/connectors/spicedb#materializedenormalize-permissions","content":" It is possible to denormalize the relations in a SpiceDB schema based on a permission in a type. This can be useful to add permissions into a search engine or similar where searching should be done based on the users permissions.  First a plan must be created:  var viewPermissionPlan = SpiceDbToFlowtide.Convert(schemaText, &quot;document&quot;, &quot;view&quot;, &quot;spicedb&quot;);   It requires the schema, which can be fetched from the schema service, or loaded for a file. The second argument is the type, and the third is the permission/relation to denormalize. The last argument is which table name should be used, and should be matched in the ReadWriteFactory.  // Add the plan as a view for sql sqlPlanBuilder.AddPlanAsView(&quot;authdata&quot;, viewPermissionPlan); // use the view in a query sqlPlanBuilder.Sql(@&quot; INSERT INTO outputtable SELECT subject_type, subject_id, relation, resource_type, resource_id FROM authdata &quot;);  ","version":"Next","tagName":"h2"},{"title":"OpenFGA Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/openfga","content":"","keywords":"","version":"Next"},{"title":"Sink​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#sink","content":" The OpenFGA sink allows you to write data into OpenFGA.  These columns are required to insert data:  user_type - user typeuser_id - identifier of the userrelation - relation nameobject_type - object typeobject_id - identifier of the object.  Optional:  user_relation - optional user relation, example in OpenFGA: 'user:1#{userRelation}'  To use the OpenFGA Sink add the following line to the ConnectorManager:  connectorManager.AddOpenFGASink(&quot;regex pattern for tablename&quot;, new OpenFGASinkOptions { ClientConfiguration = clientConfiguration });   Sql example:  INSERT INTO openfga SELECT 'user' as user_type, o.userkey as user_id, 'member' as relation, 'doc' as object_type, o.orderkey as object_id FROM orders o   ","version":"Next","tagName":"h2"},{"title":"Events​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#events","content":" The sink provides multiple event listeners to be able to extend the solution such as writing to an external store. It may be useful to write to an external store what has been written to OpenFGA, to be able to clean up what was written from a stream.  BeforeWriteFunc - Called before each write of a tuple, can be called multiple times in parallel.BeforeDeleteFunc - Called before each delete of a tuple, can be called multiple times in parallel.OnInitialDataSentFunc - Called after all initial data has been written for the first time of a stream.OnWatermarkFunc - Called after a write with the new watermarks of the source systems that have contributed to the data written.  ","version":"Next","tagName":"h3"},{"title":"Delete existing data if not updated​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#delete-existing-data-if-not-updated","content":" It is possible to delete existing data in OpenFGA if it is not in the result set of the stream. This is done by passing in the property DeleteExistingDataFetcher which should fetch existing data from OpenFGA to compare against. If your stream say updates object type document and relation reader you should return that from the fetcher if you wish to delete existing data that is not from the current stream.  This will cause all data to be downloaded into the stream which will cause a slower performance to read the initial data.  ","version":"Next","tagName":"h3"},{"title":"Source​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#source","content":" The OpenFGA source allows you to read data from OpenFGA, which can be useful to combine with other data. It can be combined with the authorization model permission to query parser which creates a view from your model for a specific permission which can then be materialized with your data.  The source is added to the ConnectorManager with the following line:  connectorManager.AddOpenFGASource(&quot;regex pattern for tablename&quot;, new OpenFGASourceOptions { ClientConfiguration = clientConfiguration });   ","version":"Next","tagName":"h2"},{"title":"Output columns​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#output-columns","content":" The source splits up the user and object columns into user_type, user_id, object_type and object_id. This is done to reduce memory consumption in the system since there is a low cardinality on the type fields, and the values can be reused.  user_type - user typeuser_id - identifier of the useruser_relation - optional user relation, example in OpenFGA: 'user:1#{userRelation}', default is null if not set.relation - relation nameobject_type - object typeobject_id - identifier of the object.  ","version":"Next","tagName":"h3"},{"title":"Authorization model permission to query​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#authorization-model-permission-to-query","content":" warning Authorization model permission to query is still experimental.  It is possible to take in a authorization model and denormalize the permission for a specific type. This can be useful if one wants to add permisison data to a non relational database such as a search engine to allow searching based on user permissions.  If one develops a CQRS service for instance the query service can get in the materialized permissions from OpenFGA into its database. It can also be useful if the permissions should be sent to another system that does not integrate with OpenFGA.  Important to note is that one should check the watermark on the stream from OpenFGA to make sure that permissions have been synced to the external system.  To use the model to query parser with sql plan builder, write the following:  var modelPlan = OpenFgaToFlowtide.Convert(yourModel, &quot;{type name}&quot;, &quot;{relation name}&quot;, &quot;{input table name}&quot;); sqlPlanBuilder.AddPlanAsView(&quot;permissions&quot;, modelPlan); // Add a source that matches the input table name for openfga connectorManager.AddOpenFGASource(&quot;{input table name}&quot;, new OpenFGASourceOptions { ClientConfiguration = clientConfiguration });   The view will contains the following columns:  user_type - user type field from the tupleuser_id - identifier of the userrelation - relation nameobject_type - object type field from the tupleobject_id - identifier of the object.  You can then use the data from the view to join it with other data.  ","version":"Next","tagName":"h2"},{"title":"Stop at types​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#stop-at-types","content":" It is possible to send in an array of type names where the search should end. This can be useful in scenarios where say an entire company has access to a resource, it can be better to add the company identifier instead of every single user in the company.  Example:  var modelPlan = OpenFgaToFlowtide.Convert(yourModel, &quot;{type name}&quot;, &quot;{relation name}&quot;, &quot;{input table name}&quot;, &quot;company&quot;);   The relation name will still be the relation name you are filtering on but instead with the object type company and its identifier.  This can work well with using OpenFGA ListObjects commands where one could list the companies a user belongs to. It does require that the application knows more about the authorization model, but can be a good optimization to avoid alot of user rows.  ","version":"Next","tagName":"h3"},{"title":"How it works​","type":1,"pageTitle":"OpenFGA Connector","url":"/flowtide/docs/connectors/openfga#how-it-works","content":" Given the following permission structure:  model schema 1.1 type user type group relations define parent: [group] define member: [user] define can_read: member or can_read from parent type doc relations define group: [group] define can_read: can_read from group   And one wants to materialize users based on &quot;doc: can_read&quot; definition, the following query graph is created:    Loop feedback will send all events back into the loop ingress. The distinct node directly after the loop ingress is to stop row duplicates to iterate endlessly, while still allowing rows a high iteration count before terminating.  The default iteration count allowed is 1000 iterations. This is set to stop any potential endless loops. ","version":"Next","tagName":"h3"},{"title":"SQL Server Connector","type":0,"sectionRef":"#","url":"/flowtide/docs/connectors/sqlserver","content":"","keywords":"","version":"Next"},{"title":"Supported Data Types​","type":1,"pageTitle":"SQL Server Connector","url":"/flowtide/docs/connectors/sqlserver#supported-data-types","content":" The SQL Server connector supports reading and writing the following data types:  IntBigIntBinaryBitCharDateDatetimeDatetime2DecimalFloatImageMoneyNcharNtextNumericNvarcharRealSmalldatetimeSmallintTextTimeTinyintUniqueidentifierVarbinaryVarcharXml  ","version":"Next","tagName":"h2"},{"title":"Source​","type":1,"pageTitle":"SQL Server Connector","url":"/flowtide/docs/connectors/sqlserver#source","content":" The SQL Server Source allows Flowtide to fetch rows and updates from a SQL Server table. There is one prerequisite for this connector to work:  info Change tracking must be enabled on the table.  Without change tracking, Flowtide wont be able to find updates on the table. There are plans to allow the source to run in batch mode where it computes the delta inside of the connector, but that is not yet available.  The SQL Server Source can be added to the 'ConnectorManager' with the following line:  connectorManager.AddSqlServerSource(() =&gt; connectionString);   The connection string must be set as a function, since the idea is that the connection string might change, from say a system such asHashicorp Vault.  The source uses the following logic to fetch data into the stream:    The source will retry fetching data in-case of a SQL Server error, as long as it can reconnect to the database. It will mark the operator as unhealthy, but it will not trigger a stream restart.  If the operator cannot reconnect to the SQL Server, it will trigger a full stream restart.  ","version":"Next","tagName":"h2"},{"title":"Sink​","type":1,"pageTitle":"SQL Server Connector","url":"/flowtide/docs/connectors/sqlserver#sink","content":" The SQL Server Sink implements the grouped write operator. This means that all rows are grouped by a primary key, thus all sink tables must have a primary key defined.  info All SQL Server Sink tables must have a primary key defined. The primary key must also be in the query that fills the table.  Its implementation waits fully until the stream has reached a steady state at a time T until it writes data to the database. This means that its table output can always be traced back to a state from the source systems.  To use the SQL Server Sink add the following line to the ConnectorManager:  connectorManager.AddSqlServerSink(() =&gt; connectionString);   As with the SQL Server Source, the connection string is returned by a function to enable dynamic connection strings.  The sink inserts data into SQL Server by creating a temporary table, which follows the table structure of the destination with an added operation metadata column. The data is inserted into the temporary table using Bulk Copy. This allows for fast binary insertion into the temporary table.  After data has been inserted into the temporary table, a merge into statement is run that merges data into the destination table. After all data has been merged, the temporary table is cleared of all data.  warning If there are multiple rows in the result with the same primary key, only the latest seen row will be inserted into the destination table.  ","version":"Next","tagName":"h2"},{"title":"Custom Primary Keys​","type":1,"pageTitle":"SQL Server Connector","url":"/flowtide/docs/connectors/sqlserver#custom-primary-keys","content":" In some scenarios you may want to override the table's primary keys or the table might not have a primary key configured. In this scenario you can provide column names for the columns you want Flowtide to use as primary keys.  Ex:  connectorManager.AddSqlServerSink(&quot;your regexp on table names&quot;, new SqlServerSinkOptions() { ConnectionStringFunc = () =&gt; connectionString, CustomPrimaryKeys = new List&lt;string&gt;() { &quot;my_column1&quot;, &quot;my_column2&quot; } });   ","version":"Next","tagName":"h3"},{"title":"SQL Table Provider​","type":1,"pageTitle":"SQL Server Connector","url":"/flowtide/docs/connectors/sqlserver#sql-table-provider","content":" The SQL table provider is added to the SQL plan builder which will try and look after used tables in its configured SQL Server. It provides metadata information about what the column names are in the table.  To use the table provider add the following line to the Sql plan builder:  sqlBuilder.AddSqlServerProvider(() =&gt; connectionString);   If you are starting Flowtide with dependency injection, a table provider is added automatically, so this step is not required. ","version":"Next","tagName":"h2"},{"title":"Arithmetic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic","content":"","keywords":"","version":"Next"},{"title":"Sum​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sum","content":" Substrait definition  Calculates the sum of numeric values, if there are no rows a NULL value is returned. if a value is non numeric such as a string or null, those values are ignored.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sql-usage","content":" SELECT sum(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Sum0​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sum0","content":" Substrait definition  Calculates the sum of numeric values, if there are no rows a 0 value is returned. if a value is non numeric such as a string or null, those values are ignored.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sql-usage-1","content":" SELECT sum0(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Min​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#min","content":" Substrait definition  Returns the minimum value in the result. If there are no rows a NULL value is returned. MIN ignores any null input values.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sql-usage-2","content":" SELECT min(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Max​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#max","content":" Substrait definition  Returns the maximum value in the result. If there are no rows a NULL value is returned. MAX ignores any null input values.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/arithmetic#sql-usage-3","content":" SELECT max(column1) FROM ...  ","version":"Next","tagName":"h3"},{"title":"List Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/aggregatefunctions/list","content":"","keywords":"","version":"Next"},{"title":"List Agg​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/aggregatefunctions/list#list-agg","content":" This function does not have a substrait definition.  List Agg creates a list of values per group. This is useful when denormalizing data. It takes in one expression which will be the value added to the list for that row.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/aggregatefunctions/list#sql-usage","content":" SELECT key1, list_agg(value1) FROM table1 GROUP BY key1   Given the following two rows:  1, 'hello' 1, 'world'  The output would be:  1, ['hello', 'world']  ","version":"Next","tagName":"h3"},{"title":"List Union Distinct Agg​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/aggregatefunctions/list#list-union-distinct-agg","content":" This function does not have a substrait definition.  List Union Distinct Agg combines multiple lists and returns the distinct set of all lists. This can be useful when having multiple rows with lists that need to be combined.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/aggregatefunctions/list#sql-usage-1","content":" SELECT key1, list_union_distinct_agg(value1) FROM table1 GROUP BY key1   Given the following two rows:  1, ['hello', 'world']1, ['world']  The output would be:  1, ['hello', 'world'] ","version":"Next","tagName":"h3"},{"title":"Pause And Resume","type":0,"sectionRef":"#","url":"/flowtide/docs/deployment/pauseresume","content":"","keywords":"","version":"Next"},{"title":"Pause and stop using IConfiguration​","type":1,"pageTitle":"Pause And Resume","url":"/flowtide/docs/deployment/pauseresume#pause-and-stop-using-iconfiguration","content":" One of the easier ways to add pause and resume is to utilize IConfiguration in .NET. Flowtide uses an IOptionsMonitor&lt;FlowtidePauseOptions&gt; to check for changes on the configuration and pauses and resumes the stream based on the value.  The class looks as follows:  public class FlowtidePauseOptions { public bool IsPaused { get; set; } }   So to utilize this when using FlowtideDotNet.AspNetCore or FlowtideDotNet.DependencyInjection you add the following:  var builder = WebApplication.CreateBuilder(args); ... // Map flowtide pause options to enable pausing and resuming from configuration builder.Services.AddOptions&lt;FlowtidePauseOptions&gt;() .Bind(builder.Configuration.GetSection(&quot;your_section&quot;)); ... // Add the stream as normal builder.Services.AddFlowtideStream(&quot;stream_name&quot;) ...   This is best fitted with an IConfiguration provider that supports loading changes dynamically such as Hashicorp Vault or Azure Key Vault. Pausing and resuming using IConfigurationprovider is dependent on the update frequency of the provider, so to utilize this fully this interval should be kept quite low.  ","version":"Next","tagName":"h2"},{"title":"Pause and stop using API endpoint​","type":1,"pageTitle":"Pause And Resume","url":"/flowtide/docs/deployment/pauseresume#pause-and-stop-using-api-endpoint","content":" When using FlowtideDotNet.AspNetCore you can also map API endpoints to allow for pause and resume.  Example:  var builder = WebApplication.CreateBuilder(args); ... // Add the stream as normal builder.Services.AddFlowtideStream(&quot;stream_name&quot;) ... var app = builder.Build(); ... // Map pause and resume endpoints app.UseFlowtidePauseResumeEndpoints(&quot;base_path&quot;);   Two endpoints are registered under the base path, /pause and /resume. ","version":"Next","tagName":"h2"},{"title":"Generic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/aggregatefunctions/generic","content":"","keywords":"","version":"Next"},{"title":"Count​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/generic#count","content":" Substrait definition  Counts all rows in a query. This function takes no parameters, the implementation of count with a column name is not yet implemented.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/generic#sql-usage","content":" SELECT count(*) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Surrogate Key Int64​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/generic#surrogate-key-int64","content":" This function does not have a substrait definition.  Generates a unique int64 value for each combination of the &quot;group by&quot; columns in an aggregate query. This can be used for instance when creating a SCD table where the group by can be on both the primary key and date.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/aggregatefunctions/generic#sql-usage-1","content":" SELECT surrogate_key_int64() as SK_MyKey, id, insertDate FROM testtable GROUP BY id, insertDate  ","version":"Next","tagName":"h3"},{"title":"String functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/aggregatefunctions/string","content":"","keywords":"","version":"Next"},{"title":"String Agg​","type":1,"pageTitle":"String functions","url":"/flowtide/docs/expressions/aggregatefunctions/string#string-agg","content":" Substrait definition  Concatenates a column of string values with a separator.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String functions","url":"/flowtide/docs/expressions/aggregatefunctions/string#sql-usage","content":" SELECT key1, string_agg(value1, ',') FROM table1 GROUP BY key1  ","version":"Next","tagName":"h3"},{"title":"Check Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/check","content":"","keywords":"","version":"Next"},{"title":"Check value​","type":1,"pageTitle":"Check Functions","url":"/flowtide/docs/expressions/scalarfunctions/check#check-value","content":" This function has no substrait equivalent  check_value takes in at least three arguments:  Scalar value - this value is passed through and returned by the function.Condition - boolean condition of the checkMessage - String message that should be logged if the check fails.  After these three arguments, any extra arguments are added as tags.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Check Functions","url":"/flowtide/docs/expressions/scalarfunctions/check#sql-usage","content":" select CHECK_VALUE(column1, column1 &lt; column2, 'Column1 is larger than column2: {column1} &gt; {column2}', column1, column2) as val FROM ...   ","version":"Next","tagName":"h3"},{"title":"Check true​","type":1,"pageTitle":"Check Functions","url":"/flowtide/docs/expressions/scalarfunctions/check#check-true","content":" This function has no substrait equivalent  check_true takes in at least two arguments:  Condition - boolean condition of the check, the result of this check is returned.Message - String message that should be logged if the check fails.  After these two arguments, any extra arguments are added as tags.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Check Functions","url":"/flowtide/docs/expressions/scalarfunctions/check#sql-usage-1","content":" select * FROM ... WHERE CHECK_TRUE(column1 &lt; column2, 'Column1 is larger than column2: {column1} &gt; {column2}', column1, column2)  ","version":"Next","tagName":"h3"},{"title":"Boolean Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/boolean","content":"","keywords":"","version":"Next"},{"title":"Or​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#or","content":" Substrait definition  Or implements the boolean logic Or operator. Its return value will always be a boolean. An argument into the Or function that is not a boolean will be the same as the boolean value false.  Implements Kleene logic with the following truth table:  +------+-------+-----+-----+-----+ | | **B** | + **A OR B** +-----+-----+-----+ | |**F**|**N**|**T**| +------+-------+-----+-----+-----+ | | **F** | F | N | T | + +-------+-----+-----+-----+ |**A** | **N** | N | N | T | + +-------+-----+-----+-----+ | | **T** | T | T | T | +------+-------+-----+-----+-----+   F = False, T = True, N = Null  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#sql-usage","content":" ... WHERE c1 = 'hello' OR c2 = 'world'   ","version":"Next","tagName":"h3"},{"title":"And​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#and","content":" Substrait definition  And implements the boolean logic And operator. Its return value will always be a boolean. An argument into the And function that is not a boolean will be the same as the boolean value false.  Implements Kleene logic with the following truth table:  +------+-------+-----+-----+-----+ | | **B** | + **A AND B** +-----+-----+-----+ | |**F**|**N**|**T**| +------+-------+-----+-----+-----+ | | **F** | F | F | F | + +-------+-----+-----+-----+ |**A** | **N** | F | N | N | + +-------+-----+-----+-----+ | | **T** | F | N | T | +------+-------+-----+-----+-----+   F = False, T = True, N = Null  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#sql-usage-1","content":" ... WHERE c1 = 'hello' AND c2 = 'world'   ","version":"Next","tagName":"h3"},{"title":"Not​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#not","content":" Substrait definition  The not of a boolean value. When a null is input, a null is output.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#sql-usage-2","content":" ... WHERE NOT c1 = 'hello'   ","version":"Next","tagName":"h3"},{"title":"Xor​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#xor","content":" Substrait definition  Xor implements the boolean logic Xor operator using kleene logic.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Boolean Functions","url":"/flowtide/docs/expressions/scalarfunctions/boolean#sql-usage-3","content":" ... WHERE (c1 = 'hello') XOR (c2 = 'world')  ","version":"Next","tagName":"h3"},{"title":"Datetime Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/datetime","content":"","keywords":"","version":"Next"},{"title":"Strftime​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#strftime","content":" Substrait definition  Converts a timestamp in microseconds since 1970-01-01 00:00:00.000000 to a string. See strftime on how to to write the format string.  Arguments:  Timestamp in microsecondsFormat as a string  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#sql-usage","content":" INSERT INTO output SELECT strftime(Orderdate, '%Y-%m-%d %H:%M:%S') as Orderdate FROM Orders   ","version":"Next","tagName":"h3"},{"title":"Get timestamp​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#get-timestamp","content":" This function has no substrait equivalent  warning Get timestamp is not yet supported inside of join conditions.  info If you do comparisons with gettimestamp it may be benificial to use buffered views to only send out changed rows to the rest of the stream. Please see Buffered view.  Get the current timestamp (current datetime).  This is a meta function that itself does no computation, but is instead replaced during the optimization step with joins against a timestamp data source.  Example:    Where if project uses gettimestamp becomes:    This means that all rows that are evaluated will have the same value of gettimestamp to ensure consistent results.  The timestamp provider also publishes a watermark called '__timestamp' which can be used to see the current time that has been evaluted in the stream.  By default the timestamp is updated every hour. This is configurable during the stream setup:  streamBuilder .SetGetTimestampUpdateInterval(TimeSpan.FromHours(12))   ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#sql-usage-1","content":" INSERT INTO output SELECT gettimestamp() as CurrentDateTime FROM Orders   ","version":"Next","tagName":"h3"},{"title":"Floor timestamp day​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#floor-timestamp-day","content":" This function has no substrait equivalent  Rounds a timestamp down to the nearest day by removing the time portion (hours, minutes, seconds, etc.). If the input is null or not of the type timestamp, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Datetime Functions","url":"/flowtide/docs/expressions/scalarfunctions/datetime#sql-usage-2","content":" SELECT floor_timestamp_day(timestamp_column) FROM ...  ","version":"Next","tagName":"h3"},{"title":"Comparison Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/comparison","content":"","keywords":"","version":"Next"},{"title":"Equal​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#equal","content":" Substrait definition  Compares equality of two values. If the two values have different types they are not considered equal, so a float with value 1 will not equal an integer with value 1. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage","content":" ... WHERE c1 = 'hello'   ","version":"Next","tagName":"h3"},{"title":"Not equal​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#not-equal","content":" Substrait definition  Checks two values for non equality. Different types will immedietly return true, that the values are not equal. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-1","content":" ... WHERE c1 != 'hello'   ","version":"Next","tagName":"h3"},{"title":"Greater than​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#greater-than","content":" Substrait definition  Checks if the left value is greater than the right value. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-2","content":" ... WHERE c1 &gt; 1   ","version":"Next","tagName":"h3"},{"title":"Greater than or equal​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#greater-than-or-equal","content":" Substrait definition  Checks if the left value is greater than or equal to the right value. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-3","content":" ... WHERE c1 &gt;= 1   ","version":"Next","tagName":"h3"},{"title":"Less than​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#less-than","content":" Substrait definition  Checks if the left value is less than the right value. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-4","content":" ... WHERE c1 &lt; 1   ","version":"Next","tagName":"h3"},{"title":"Less than or equal​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#less-than-or-equal","content":" Substrait definition  Checks if the left value is less than or equal to the right value. If any argument is null, the result is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-5","content":" ... WHERE c1 &lt;= 1   ","version":"Next","tagName":"h3"},{"title":"Between​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#between","content":" Substrait definition  Checks if an expression is between two values.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-6","content":" ... WHERE c1 BETWEEN 100 AND 200   ","version":"Next","tagName":"h3"},{"title":"Is not null​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#is-not-null","content":" Substrait definition  Checks if a single argument is not equal to null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-7","content":" ... WHERE c1 is not null   ","version":"Next","tagName":"h3"},{"title":"Is Null​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#is-null","content":" Substrait definition  Checks if a signle argument is null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-8","content":" ... WHERE c1 is null   ","version":"Next","tagName":"h3"},{"title":"Coalesce​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#coalesce","content":" Substrait definition  Returns the first value, left to right that is not equal to null. If all values are null, a null value is returned.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-9","content":" SELECT coalesce(column1, column2) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Is Infinite​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#is-infinite","content":" Substrait definition  Checks if a numeric value is positive or negative infinite. If the value is NaN (0 / 0), or another type, it returns false.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-10","content":" SELECT is_infinite(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Is Finite​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#is-finite","content":" Substrait definition  Checks if a numeric value is not positive or negative infinite or NaN. If the value is not numeric it returns false.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-11","content":" SELECT is_finite(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Is NaN​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#is-nan","content":" Substrait definition  Checks if an exprssion is not a numeric value. A null value returns null as in the substrait definition.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-12","content":" SELECT is_nan(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Greatest​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#greatest","content":" Substrait definition  Returns the largest non-null value from a list of input expressions. If all values are null, the result is null. Follows the behavior defined in the Substrait specification.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Comparison Functions","url":"/flowtide/docs/expressions/scalarfunctions/comparison#sql-usage-13","content":" SELECT greatest(column1, column2, column3) FROM ...  ","version":"Next","tagName":"h3"},{"title":"List Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/list","content":"","keywords":"","version":"Next"},{"title":"List Sort Ascending Null Last​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/scalarfunctions/list#list-sort-ascending-null-last","content":" This function has no substrait equivalent  Sorts a list of values in ascending order, placing any null values at the end of the result.  Values are ordered according to their natural ascending order (e.g., numerically or lexicographically). Nulls are not compared to other values directly; they are always considered greater for the purpose of ordering. Any value that is not a list will return the result as null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/scalarfunctions/list#sql-usage","content":" SELECT list_sort_asc_null_last(list(orderkey, userkey)) FROM ...   ","version":"Next","tagName":"h3"},{"title":"List First Difference​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/scalarfunctions/list#list-first-difference","content":" This function has no substrait equivalent  Returns the first element from the first list where a difference is found when compared to the corresponding element in the second list. The comparison is done element-by-element, maintaining the order of the lists. If no difference is found (i.e., all elements in both lists are equal, or the first list is shorter than the second list), the function returns NULL.  If one or both of the lists are NULL, the function handles them as follows:  If the second list is NULL, the function returns the first element of the first list (if it exists).If the first list is NULL, the function returns NULL.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"List Functions","url":"/flowtide/docs/expressions/scalarfunctions/list#sql-usage-1","content":" SELECT list_first_difference(list1, list2) FROM ...  ","version":"Next","tagName":"h3"},{"title":"Rounding Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/rounding","content":"","keywords":"","version":"Next"},{"title":"Ceiling​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#ceiling","content":" Substrait definition  Rounds a number up to its closest integer.  Its output type will always be an integer, if a non numeric type is passed in, the function will return null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#sql-usage","content":" SELECT ceiling(column1) FROM ...   or  SELECT ceil(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Floor​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#floor","content":" Substrait definition  Rounds a number down to its closest integer.  Its output type will always be an integer, if a non numeric type is passed in, the function will return null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#sql-usage-1","content":" SELECT floor(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Round​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#round","content":" Substrait definition  Rounds a number to its closest integer.  Its output type will always be an integer, if a non numeric type is passed in, the function will return null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Rounding Functions","url":"/flowtide/docs/expressions/scalarfunctions/rounding#sql-usage-2","content":" SELECT round(column1) FROM ...  ","version":"Next","tagName":"h3"},{"title":"Table Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/tablefunctions","content":"Table Functions Table functions are specialized functions that returns multiple rows. An example of a table function is 'UNNEST' which takes in a list of data and returns one row for each element in the list. In Flowtide, table functions are used with the TableFunctionRelation. There is no definition yet in substrait for table functions and its relations, so a custom definition is created for flowtide. It uses the ExtensionSingleRel if used with an input. The input is only required when it is used in joins. If no input is used, ExtensionLeafRel can be used instead. If the table function is a root, for instance in SQL: ... FROM func(arg) No input is required. The following proto definition is used to define the TableFunctionRelation: message TableFunction { // Points to a function_anchor defined in this plan, which must refer // to a table function in the associated YAML file. Required; 0 is // considered to be a valid anchor/reference. uint32 function_reference = 1; // Schema of the output table from the function substrait.NamedStruct table_schema = 2; // Arguments for the table function repeated substrait.FunctionArgument arguments = 7; } message TableFunctionRelation { // Table function to use TableFunction table_function = 1; // Only required if used with an input. // Only left and inner join supported at this time. JoinType type = 6; // Optional, but only usable with an input. substrait.Expression join_condition = 4; enum JoinType { JOIN_TYPE_UNSPECIFIED = 0; JOIN_TYPE_INNER = 1; JOIN_TYPE_LEFT = 3; } } See below for the different table functions: 📄️ Generic Functions Unnest","keywords":"","version":"Next"},{"title":"Generic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/tablefunctions/generic","content":"","keywords":"","version":"Next"},{"title":"Unnest​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/tablefunctions/generic#unnest","content":" There is not substrait definition for unnest  Extension URI: /functions_table_generic.yamlExtension Name: unnest  Unnest is a table function which takes in one argument that can be a list of element or a map. If it is a list then it returns one row for each entry in the list. If it is a map, it will return an object with that looks as the following:  { key: &quot;fieldName&quot;, value: fieldValue }   If it is a list, it will return the element as it was in the list.  An example with a list would be the list [1, &quot;test&quot;, 3] returns three rows with values:  Value1 &quot;test&quot; 3  In the case of an object { field1: &quot;test&quot;, field2: &quot;test2&quot; } it would return:  Value{ key: &quot;field1&quot;, value: &quot;test&quot; } { key: &quot;field2&quot;, value: &quot;test2&quot; }  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/tablefunctions/generic#sql-usage","content":" In a FROM statement​  --- Returns three rows with 1, 2, 3 as val SELECT val FROM UNNEST(list(1,2,3)) val   With a left join​  SELECT id, element_value FROM documents d LEFT JOIN UNNEST(d.list) element_value   When used in a LEFT JOIN, rows are still returned even if the list is empty.  Left join with a condition​  SELECT id, element_value FROM documents d LEFT JOIN UNNEST(d.list) element_value ON element_value = 123   In the above example all rows are returned but element_value is only set if it is equal to 123.  Inner join​  SELECT id, element_value FROM documents d INNER JOIN UNNEST(d.list) element_value   When used in an INNER JOIN, only rows that have elements in the list will be returned. Inner join works the same with conditions as left joins, but rows are not returned with a null value if not matched to the condition. ","version":"Next","tagName":"h3"},{"title":"Window Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/windowfunctions","content":"Window Functions Window functions compute values over a group of rows that are somehow related to the current row—commonly referred to as a window. Unlike aggregate functions, window functions do not collapse rows, making them ideal for tasks like ranking, running totals, or accessing neighboring values. Each window function defines how it processes rows within its partition, and may consider ordering, frame boundaries, and more. Browse the available window functions below to learn more about their behavior and usage.","keywords":"","version":"Next"},{"title":"Arithmetic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic","content":"","keywords":"","version":"Next"},{"title":"Add​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#add","content":" Substrait definition  Add takes two parameters and does an addition of the two values.  Add depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tInteger Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  Only numeric inputs will return a result, otherwise it will return null.  ","version":"Next","tagName":"h2"},{"title":"SQL usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage","content":" In SQL the add function is called using the plus operator:  SELECT column1 + 13 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Subtract​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#subtract","content":" Substrait definition  Subtract takes two parameters and does a subtraction of the two values.  Subtract depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tInteger Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  Only numeric inputs will return a result, otherwise it will return null.  ","version":"Next","tagName":"h2"},{"title":"SQL usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-1","content":" In SQL the subtract function is called using the minus operator:  SELECT column1 - 13 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Multiply​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#multiply","content":" Substrait definition  Multipies two numbers.  Multiply depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tInteger Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-2","content":" SELECT column1 * 3 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Divide​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#divide","content":" Substrait definition  Divide two numbers.  Divide depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tFloat Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  There are some special cases when dividing with zero:  0 / 0 -&gt; this results in NaN.PositiveNumber / 0 -&gt; +InfinityNegativeNumber / 0 -&gt; -Infinity  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-3","content":" SELECT column1 / 3 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Negate​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#negate","content":" Substrait definition  Negates a numeric value, example:  1 becomes -1-1 becomes 11.3 becomes -1.3  Non numeric values becomes 'null'.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-4","content":" SELECT -column1 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Modulo​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#modulo","content":" Substrait definition  Calculate the remainder when dividing two numbers.  Modulo depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tInteger Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  Taking modulo between two integers where the divider is 0, the return will be type double and the value NaN.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-5","content":" SELECT column1 % 3 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Power​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#power","content":" Substrait definition  Calculate the power with the first argument being the base and the other the exponent.  Power depends on the input types on what result it will give:  Left type\tRight type\tOutputInteger\tInteger\tInteger Integer\tFloat\tFloat Float\tFloat\tFloat Decimal\tInteger\tDecimal Decimal\tFloat\tDecimal Decimal\tDecimal\tDecmial Non numeric\tInteger\tNull num numeric\tFloat\tNull num numeric\tDecimal\tNull Non numeric\tNon numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-6","content":" SELECT power(column1, 2) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Sqrt​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sqrt","content":" Substrait definition  Calculate the square root of a number.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tDecimal Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-7","content":" SELECT sqrt(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Exp​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#exp","content":" Substrait definition  Calculates the constant e raised to the power of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-8","content":" SELECT exp(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Cos​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#cos","content":" Substrait definition  Calculate the cosine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-9","content":" SELECT cos(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Sin​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sin","content":" Substrait definition  Calculate the sine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-10","content":" SELECT sin(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Tan​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#tan","content":" Substrait definition  Calculate the tangent of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-11","content":" SELECT tan(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Cosh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#cosh","content":" Substrait definition  Calculate the hyperbolic cosine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-12","content":" SELECT cosh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Sinh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sinh","content":" Substrait definition  Calculate the hyperbolic sine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-13","content":" SELECT sinh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Tanh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#tanh","content":" Substrait definition  Calculate the hyperbolic tangent of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-14","content":" SELECT tanh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Acos​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#acos","content":" Substrait definition  Calculate the arccosine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-15","content":" SELECT acos(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Asin​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#asin","content":" Substrait definition  Calculate the arcsine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-16","content":" SELECT asin(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Atan​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#atan","content":" Substrait definition  Calculate the arctangent of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-17","content":" SELECT atan(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Acosh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#acosh","content":" Substrait definition  Calculate the hyperbolic arccosine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-18","content":" SELECT acosh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Asinh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#asinh","content":" Substrait definition  Calculate the hyperbolic arcsine of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-19","content":" SELECT asinh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Atanh​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#atanh","content":" Substrait definition  Calculate the hyperbolic arctangent of the input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-20","content":" SELECT atanh(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Atan2​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#atan2","content":" Substrait definition  Calculate the arctangent of two input value.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-21","content":" SELECT atan2(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Radians​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#radians","content":" Substrait definition  Convert the input value from degrees to radians.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-22","content":" SELECT radians(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Degrees​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#degrees","content":" Substrait definition  Convert the input value from radians to degrees.  Output types:  Type\tOutputInteger\tFloat Float\tFloat Decimal\tFloat Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-23","content":" SELECT degrees(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Abs​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#abs","content":" Substrait definition  Calculate the absolute value of the input value.  Output types:  Type\tOutputInteger\tInteger Float\tFloat Decimal\tDecimal Non numeric\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-24","content":" SELECT abs(column1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Sign​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sign","content":" Substrait definition  Get the sign of the input value.  Output:  Type\tOutput Type\tOutput rangeInteger\tInteger\t[-1, 0, 1] Float\tFloat\t[-1.0, 0.0, 1.0, NaN] Decimal\tDecimal\t[-1.0, 0.0, 1.0] Non numeric\tNull\tNull  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/scalarfunctions/arithmetic#sql-usage-25","content":" SELECT sign(column1) FROM ...  ","version":"Next","tagName":"h3"},{"title":"String Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/scalarfunctions/string","content":"","keywords":"","version":"Next"},{"title":"Concat​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#concat","content":" Substrait definition  Concatinates two or more string values together.  This function tries and convert non string values into the string type, example:  Input\tType\tOutput'hello'\tString\t'hello' 13\tInt\t'13' 13.4\tFloat\t'13.4' true\tBool\t'true'  If any argument is null, the return value will always be null.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage","content":" SELECT c1 || ' hello ' || c2 FROM ...   ","version":"Next","tagName":"h3"},{"title":"Lower​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#lower","content":" Substrait definition  Returns the input string in all lowercase characters. If any other type than string is entered, the function will return 'null'.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-1","content":" SELECT lower(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Upper​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#upper","content":" Substrait definition  Returns the input string in all uppercase characters. If any other type than string is entered, the function will return 'null'.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-2","content":" SELECT upper(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Trim​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#trim","content":" Substrait definition  Remove whitespaces from both sides of a string  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-3","content":" SELECT trim(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"LTrim​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#ltrim","content":" Substrait definition  Remove whitespaces from the start of a string  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-4","content":" SELECT ltrim(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"RTrim​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#rtrim","content":" Substrait definition  Remove whitespaces from the end of a string  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-5","content":" SELECT rtrim(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"To String​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#to-string","content":" No substrait definition exists for this function  Converts different types to a string type.  Example output:  Input\tType\tOutput'hello'\tString\t'hello' 13\tInt\t'13' 13.4\tFloat\t'13.4' true\tBool\t'true'  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-6","content":" SELECT to_string(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Starts with​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#starts-with","content":" Substrait definition  Returns true or false if a string starts with another string. If the data type of either argument is not string, false will be returned.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-7","content":" SELECT starts_with(c1, 'text') FROM ...   ","version":"Next","tagName":"h3"},{"title":"Substring​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#substring","content":" Substrait definition  Returns a substring where the first argument is the input. The second argument is the start index, and an optional third argument is the length of the substring.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-8","content":" SELECT substring(c1, 1) FROM ... SELECT substring(c1, 1, 5) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Like​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#like","content":" Substrait definition  Implements sql like expression. This follows SQL Servers implementation of like:SQL Server Like.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-9","content":" ... WHERE c1 LIKE '%te' -- starts with ... WHERE c1 LIKE 'te%' -- ends with ... WHERE c1 LIKE '%te%' -- contains ... WHERE c1 LIKE '_te' -- any character ... WHERE c1 LIKE '!_te' ESCAPE '!' -- set escape character, _ is escaped. ... WHERE c1 LIKE '[ab]te' -- match character a or b.   ","version":"Next","tagName":"h3"},{"title":"Replace​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#replace","content":" Substrait definition  Replaces all occurences of the substring defined in the second variable with the value defined in the third argument for the string in the first argument.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-10","content":" --- Replaces the word 'hello' with the word 'world' in the string c1. SELECT replace(c1, 'hello', 'world') FROM ...   ","version":"Next","tagName":"h3"},{"title":"String Base64 Encode​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#string-base64-encode","content":" Accepts a string as a parameter and will return the base64 encoding as a string.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-11","content":" SELECT string_base64_encode(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"String Base64 Decode​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#string-base64-decode","content":" Accepts a string in base64 encoding and will return a decoded string.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-12","content":" SELECT string_base64_decode(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Char Length​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#char-length","content":" Substrait definition  Returns the number of characters in a string, if the input is not a string or null, a null value is returned.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-13","content":" SELECT LEN(c1) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Strpos​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#strpos","content":" Substrait definition  Finds the index of a substring in another string. This function follows the substrait implementation and returns index 1 for the first character.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-14","content":" SELECT strpos(c1, 'abc') FROM ...   ","version":"Next","tagName":"h3"},{"title":"String split​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#string-split","content":" Substrait definition  Splits a string into a collection of substrings based on the specified delimiter character.  If the provided delimiter character is null, the original string will be returned as the only element in the resulting collection.  If the provided delimiter character is not a string, null will be returned.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-15","content":" SELECT string_split('a b', ' ') ...   ","version":"Next","tagName":"h3"},{"title":"Regexp string split​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#regexp-string-split","content":" Substrait definition  Splits a string into a collection of substrings based on the specified pattern.  If any of the arguments is not string, null will be returned.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-16","content":" SELECT regexp_string_split('a b', '\\s') ...   ","version":"Next","tagName":"h3"},{"title":"To Json​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#to-json","content":" This function does not have a substrait definition.  Converts an object into json stored as a string.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-17","content":" SELECT to_json(column1) ...   ","version":"Next","tagName":"h3"},{"title":"From Json​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#from-json","content":" This function does not have a substrait definition.  Converts a JSON string to flowtide data objects. It is also possible to use a binary value in utf8 encoding as the input.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"String Functions","url":"/flowtide/docs/expressions/scalarfunctions/string#sql-usage-18","content":" SELECT from_json(myjsoncolumn) ...  ","version":"Next","tagName":"h3"},{"title":"Specialized Expressions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/specializedexpressions","content":"","keywords":"","version":"Next"},{"title":"Cast Expression​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-expression","content":" The cast expression allows conversion between data types. The supported data types at this point are:  StringBooleanIntDoubleDecimal  If it is not possible to cast a value, null will be returned.  ","version":"Next","tagName":"h2"},{"title":"Cast to string​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-to-string","content":" Data Type\tValue\tOutputInteger\t3\t'3' Float\t3.1\t'3.1' Decimal\t3.1\t'3.1' Boolean\tTrue\t'true' Boolean\tFalse\t'false'  SQL Usage​  CAST(column1 AS string)   ","version":"Next","tagName":"h3"},{"title":"Cast to boolean​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-to-boolean","content":" For numeric types, any value except 0 becomes true, and 0 becomes false.  Data Type\tValue\tOutputInteger\t3\ttrue Integer\t0\tfalse Float\t3.1\ttrue Float\t0.0\tfalse Decimal\t3.1\ttrue Decimal\t0.0\tfalse String\t'true'\ttrue String\t'false'\tfalse  SQL Usage​  CAST(column1 AS boolean)   ","version":"Next","tagName":"h3"},{"title":"Cast to integer​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-to-integer","content":" For any numeric type with decimals, the value will be floored.  Data Type\tValue\tOutputFloat\t3.1\t3 Decimal\t3.1\t3 Boolean\tTrue\t1 Boolean\tFalse\t0 String\t'1'\t1  SQL Usage​  CAST(column1 AS int)   ","version":"Next","tagName":"h3"},{"title":"Cast to double​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-to-double","content":" Data Type\tValue\tOutputInt\t3\t3 Decimal\t3.1\t3.1 Boolean\tTrue\t1.0 Boolean\tFalse\t0.0 String\t'1.3'\t1.3  SQL Usage​  CAST(column1 AS double)   ","version":"Next","tagName":"h3"},{"title":"Cast to decimal​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#cast-to-decimal","content":" Data Type\tValue\tOutputInt\t3\t3 Float\t3.1\t3.1 Boolean\tTrue\t1.0 Boolean\tFalse\t0.0 String\t'1.3'\t1.3  SQL Usage​  CAST(column1 AS decimal)   ","version":"Next","tagName":"h3"},{"title":"Nested Type Constructor Expressions​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#nested-type-constructor-expressions","content":" ","version":"Next","tagName":"h2"},{"title":"List​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#list","content":" Allows the creation of a list object.  SQL Usage​  SELECT list(col1, col2) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Map​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#map","content":" Allows the creation of a map object type. A map is a typical 'json' object with property names and values. The map function consists of a list of key value pairs.  SQL Usage​  The SQL function expects an even number of arguments, the first argument is the key and the second the value for the first key value pair. The third argument is the second pairs key, etc.  SELECT map('keyvalue', col1) FROM ... SELECT map(col2, col1) FROM ...   The keys will be converted into string. A null value will result in 'null' as the key.  ","version":"Next","tagName":"h3"},{"title":"Named Struct​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#named-struct","content":" Allows the creation of a named struct object type. Struct is a fixed schema data type with a predefined set of named fields. Each field has a defined order, making it ideal for structured and predictable data.  There is a limit of a maximum of 127 different types in a union column, which means you cannot have endlessly different struct types in the same column. If dynamic properties are required, a map is a better choice.  The keys must be string literals when creating the struct.  SQL Usage​  SELECT named_struct('key1', value1, 'key2', value2) FROM ...   ","version":"Next","tagName":"h3"},{"title":"If Expression​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#if-expression","content":" Substrait definition  An if statement, or in SQL language a case statement.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#sql-usage-8","content":" SELECT CASE WHEN c1 = 'hello' THEN 1 WHEN c1 = 'world' THEN 2 ELSE 3 END FROM ...   ","version":"Next","tagName":"h3"},{"title":"Or List Expression​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#or-list-expression","content":" Substrait definition  Checks if a value is equal to any value in a list. This uses Kleene logic for equality.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Specialized Expressions","url":"/flowtide/docs/expressions/specializedexpressions#sql-usage-9","content":" ... WHERE column1 IN (1, 5, 17)  ","version":"Next","tagName":"h3"},{"title":"Arithmetic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/windowfunctions/arithmetic","content":"","keywords":"","version":"Next"},{"title":"Sum​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#sum","content":" Substrait definition  Calculates the sum of numeric values within a window. If there are no rows in the window, the result is NULL. Non-numeric values (such as strings or NULLs) are ignored during calculation.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#sql-usage","content":"  -- Total sum of all rows SELECT SUM(column1) OVER () FROM ... -- Total sum of each partition SELECT SUM(column1) OVER (PARTITION BY column2) FROM ... -- Sum from start to current row SELECT SUM(column1) OVER (PARTITION BY column2 ORDER BY column3) FROM ... -- Sum of the previous four rows and current row SELECT SUM(column1) OVER (PARTITION BY column2 ORDER BY column3 ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Row number​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#row-number","content":" Substrait definition  The ROW_NUMBER window function assigns a unique, sequential number to each row within a partition of the result set. The numbering starts at 1 and is determined by the ORDER BY clause.  This function requires an ORDER BY clause to determine row position and does not support frame boundaries.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#sql-usage-1","content":" SELECT ROW_NUMBER(column1) OVER (PARTITION BY column2 ORDER BY column3) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Lead​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#lead","content":" Substrait definition  The LEAD window function provides access to a subsequent row’s value within the same result set partition. It returns the value of a specified column at a given offset after the current row.  If no row exists at that offset, a default value (if provided) is returned; otherwise, the result is NULL.  This function requires an ORDER BY clause to establish row sequence and does not support frame boundaries.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#sql-usage-2","content":" -- Lead with default offset 1 and null default SELECT LEAD(column1) OVER (PARTITION BY column2 ORDER BY column3) FROM ... -- Lead with offset 2 and null default SELECT LEAD(column1, 2) OVER (PARTITION BY column2 ORDER BY column3) FROM ... -- Lead with offset 2 and default value set to 'hello' SELECT LEAD(column1, 2, 'hello') OVER (PARTITION BY column2 ORDER BY column3) FROM ...   ","version":"Next","tagName":"h3"},{"title":"Lag​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#lag","content":" Substrait definition  The LAG window function provides access to a preceding row’s value within the same result set partition. It returns the value of a specified column at a given offset before the current row.  If no row exists at that offset (e.g., you're at the start of the partition), a default value (if provided) is returned; otherwise, the result is NULL.  This function requires an ORDER BY clause to establish row sequence and does not support frame boundaries.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Arithmetic Functions","url":"/flowtide/docs/expressions/windowfunctions/arithmetic#sql-usage-3","content":" -- Lag with default offset 1 and null default SELECT LAG(column1) OVER (PARTITION BY column2 ORDER BY column3) FROM ... -- Lag with offset 2 and null default SELECT LAG(column1, 2) OVER (PARTITION BY column2 ORDER BY column3) FROM ... -- Lag with offset 2 and default value set to 'hello' SELECT LAG(column1, 2, 'hello') OVER (PARTITION BY column2 ORDER BY column3) FROM ...  ","version":"Next","tagName":"h3"},{"title":"Generic Functions","type":0,"sectionRef":"#","url":"/flowtide/docs/expressions/windowfunctions/generic","content":"","keywords":"","version":"Next"},{"title":"Surrogate Key Int64​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/windowfunctions/generic#surrogate-key-int64","content":" This function does not have a substrait definition.  Generates a unique int64 value for each combination of the &quot;partition by&quot; columns in an window function query. This can be used for instance when creating a SCD table where the partition by can be on both the primary key and date.  This is non-deterministic across replays of a full rerun of a stream.  ","version":"Next","tagName":"h2"},{"title":"SQL Usage​","type":1,"pageTitle":"Generic Functions","url":"/flowtide/docs/expressions/windowfunctions/generic#sql-usage","content":" SELECT surrogate_key_int64() OVER (PARTITION BY id, insertDate) as SK_MyKey, id, insertDate FROM testtable  ","version":"Next","tagName":"h3"},{"title":"Project Structure","type":0,"sectionRef":"#","url":"/flowtide/docs/internal/projectstructure","content":"Project Structure The project is structured into 6 areas: Base - Contains the basic stream implementation with checkpointing system and the base vertice types.Core - Contains the operators such as join, projection etc, and compute expression compilers.Storage - The storage solution which handles the B+ tree implementation, persistent storage, LRU cache, etc.Substrait - Handles query parsing, and creating a substrait execution plan from SQL.AspNetCore - Web implementation for monitoring of the stream, contains the time series database used for monitoring during debugging.Connectors - All the different connectors such as SQL Server, MongoDB, etc.","keywords":"","version":"Next"},{"title":"Storage","type":0,"sectionRef":"#","url":"/flowtide/docs/internal/storage","content":"Storage 📄️ Persistent Storage Rules This section describes the rules that must be upheld for persistent storage. 📄️ B+ Tree The B+ tree is used in almost all operators that require state management of rows, for instance in a join which must keep track of rows from the left input and right input. 📄️ Object State The object state is the simplest type of state for an operator, it saves a C# object using JSON serialization to store it in persistent storage.","keywords":"","version":"Next"},{"title":"Object State","type":0,"sectionRef":"#","url":"/flowtide/docs/internal/storage/objectstate","content":"Object State The object state is the simplest type of state for an operator, it saves a C# object using JSON serialization to store it in persistent storage. An object state can be fetched from the IStateManagerClient using GetOrCreateObjectStateAsync&lt;T&gt;(string name). Example: protected override async Task InitializeOrRestore(long restoreTime, IStateManagerClient stateManagerClient) { _state = await stateManagerClient.GetOrCreateObjectStateAsync&lt;MyState&gt;(&quot;my_state&quot;); } protected override async Task OnCheckpoint() { // Commit any changes made to the state await _state.Commit(); } public void OtherMethod() { _state.Value.Test = &quot;hello&quot;; } The object state saves an internal copy of the value that was last commited and checkpointed, if the value has not changed nothing will be written to persistent storage. So a user of the object state does not have to handle conditional calls to Commit to reduce the number of writes to persistent storage.","keywords":"","version":"Next"},{"title":"B+ Tree","type":0,"sectionRef":"#","url":"/flowtide/docs/internal/storage/bplustree","content":"","keywords":"","version":"Next"},{"title":"Upsert​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#upsert","content":" Upsert is used to insert or update data.  Example:  // if the tree has int as key, and string as value await tree.Upsert(1, &quot;Hello&quot;);   ","version":"Next","tagName":"h2"},{"title":"Delete​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#delete","content":" Deletes the data for a key.  Example:  await tree.Delete(1);   ","version":"Next","tagName":"h2"},{"title":"Read-Modify-Write (RMW)​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#read-modify-write-rmw","content":" Allows reading and then modifiying the data, this can result in a 'none', 'upsert' or 'delete' operation.  Example:  await tree.RMW(1, &quot;hello&quot;, (inputValue, currentValue, found) =&gt; { if (found &amp;&amp; inputValue == null) { return (default, GenericWriteOperation.Delete); } return (inputValue, GenericWriteOperation.Upsert); });   ","version":"Next","tagName":"h2"},{"title":"Get Value​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#get-value","content":" Returns the value for a key.  Example:  var (found, value) = await tree.GetValue(1);   ","version":"Next","tagName":"h2"},{"title":"Iterating over the values​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#iterating-over-the-values","content":" Since this is a B+ tree, one of the main uses is to iterate over the values in the tree. This is done with the CreateIterator method.  var iterator = tree.CreateIterator();   There are three methods on the iterator, SeekFirst which finds the most left value, Seek locates the position of a key, and Reset which resets the iterator.  Full example:  var iterator = tree.CreateIterator(); await iterator.Seek(3); // Iterate over each page, this is async since it might fetch data from persistent storage. await foreach(var page in iterator) { // Iterate over the key values in that page foreach (var keyValuePair in page) { } }   ","version":"Next","tagName":"h2"},{"title":"Commit​","type":1,"pageTitle":"B+ Tree","url":"/flowtide/docs/internal/storage/bplustree#commit","content":" When data has been written, it is not yet persisted. To persist the data one must call Commit. This is done in the OnCheckpoint method in an operator. But if the tree is used to store temporary data, Commit should not be called.  Example:  public override async Task&lt;OperatorState&gt; OnCheckpoint() { await _tree.Commit(); return new OperatorState(); }  ","version":"Next","tagName":"h2"},{"title":"Azure Monitor","type":0,"sectionRef":"#","url":"/flowtide/docs/monitoring/azuremonitor","content":"","keywords":"","version":"Next"},{"title":"Metrics export​","type":1,"pageTitle":"Azure Monitor","url":"/flowtide/docs/monitoring/azuremonitor#metrics-export","content":" To export metrics information, you need to install the following nuget packages:  OpenTelemetry.Extensions.HostingAzure.Monitor.OpenTelemetry.Exporter  Next in your Program.cs add the following code:  builder.Services.AddOpenTelemetry() .WithMetrics(builder =&gt; { builder.AddAzureMonitorMetricExporter(o =&gt; { o.ConnectionString = &quot;{your connection string}&quot;; }); builder.AddView((instrument) =&gt; { return new MetricStreamConfiguration() { Name = $&quot;{instrument.Meter.Name}.{instrument.Name}&quot; }; }); builder.AddMeter(&quot;flowtide.*&quot;); });   Replace {your connection string} with your application insights connection string. This will then start uploading custom metrics to your Application Insights in Azure Monitor.  ","version":"Next","tagName":"h2"},{"title":"Health check export​","type":1,"pageTitle":"Azure Monitor","url":"/flowtide/docs/monitoring/azuremonitor#health-check-export","content":" If you want to publish/export health check information, install the following nuget package:  AspNetCore.HealthChecks.Publisher.ApplicationInsights  Add the following to your Program.cs:  builder.Services.AddHealthChecks() .AddFlowtideCheck() .AddApplicationInsightsPublisher(&quot;{your connection string}&quot;);   Replace {your connection string} with your application insights connection string. You should now see custom events being published to Application Insights with health check information.  ","version":"Next","tagName":"h2"},{"title":"Sample​","type":1,"pageTitle":"Azure Monitor","url":"/flowtide/docs/monitoring/azuremonitor#sample","content":" A sample application exist for both setups in github. ","version":"Next","tagName":"h2"},{"title":"Persistent Storage Rules","type":0,"sectionRef":"#","url":"/flowtide/docs/internal/storage/persistentrules","content":"","keywords":"","version":"Next"},{"title":"Pages​","type":1,"pageTitle":"Persistent Storage Rules","url":"/flowtide/docs/internal/storage/persistentrules#pages","content":" Each page key should be managed exclusively by a single IPersistentStorageSession; cross-session usage is not allowed.An IPersistentStorageSession must always be used in a single-threaded context; multi-threaded calls are not permitted.Each page key should be written to persistent storage only once per checkpoint to minimize unnecessary network traffic.After a Commit operation, all page must be immediately available for read operations and return the written values, ensuring no eventual consistency delays.Data must be ensured to be persisted when CheckpointAsync returns, but the storage solution can return in-memory data before that to comply with rule 4.If any page can not be written, an exception must be thrown. It is allowed to retry before throwing.  ","version":"Next","tagName":"h2"},{"title":"Recovery​","type":1,"pageTitle":"Persistent Storage Rules","url":"/flowtide/docs/internal/storage/persistentrules#recovery","content":" The system must always be able to restore to the exact state of the last checkpoint.Any pages written must be restored to their previous value when recovering to the previous checkpoint.Deleted pages must be restored with their previous value when recovering to the previous checkpoint.Recovery only needs to support the most recent successful checkpoint; there is no requirement to support rollbacks to earlier checkpoints.  ","version":"Next","tagName":"h2"},{"title":"Checkpointing​","type":1,"pageTitle":"Persistent Storage Rules","url":"/flowtide/docs/internal/storage/persistentrules#checkpointing","content":" Only one checkpoint will be active at a time. Only call the Commit method for data that must be peristed when the OnCheckpoint method is invoked on an operator. ","version":"Next","tagName":"h2"},{"title":"General Metrics","type":0,"sectionRef":"#","url":"/flowtide/docs/monitoring/generalmetrics","content":"","keywords":"","version":"Next"},{"title":"Standard labels​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#standard-labels","content":" Label Name\tScope\tDescriptionstream\tStream\tName of the stream, exist on all metrics for a stream. operator\tOperator\tId of an operator, exist on all metrics that are specific for an operator  ","version":"Next","tagName":"h2"},{"title":"flowtide_state​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_state","content":" Scope: StreamType: Gauge  Contains the current state of the stream, only contains the stream label, the value is an integer that follows the stream state enum.  Value table:  Value\tName\tDescription0\tNotStarted\tThe stream has either never started or has been stopped. 1\tStarting\tThe stream is starting up, occurs from not started or from a failure state. 2\tRunning\tThe stream is running normally. 3\tFailure\tA failure has happened and the stream will try and recover. 4\tDeleting\tThe stream is currently deleting itself. 5\tDeleted\tThe stream has finished deleting itself. 6\tStopping\tThe stream is currently being stopped.  ","version":"Next","tagName":"h2"},{"title":"flowtide_wanted_state​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_wanted_state","content":" Scope: StreamType: Gauge  Contains the wanted state of the stream, as an example, when one stops a stream, the wanted state is NotStarted, but the current state might be Stopping. Uses the same value table as described in flowtide_state.  ","version":"Next","tagName":"h2"},{"title":"flowtide_health​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_health","content":" Scope: Stream and OperatorType: Gauge  The health of both the stream and operators, if no operator label exist, it is the overal stream health. The value can be '0' (Unhealthy), '0.5' (Degraded) and '1' (Healthy).  ","version":"Next","tagName":"h2"},{"title":"flowtide_metadata​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_metadata","content":" Scope: OperatorType: Gauge  The metadata metric only exposes labels and the value can be ignored. It exists on all operators.  Label Name\tExample\tDescriptionlinks\t[1, 2]\tJSON array of operator ids that this operator sends it output to. title\tMerge Join\tTitle/Display name of the operator.  ","version":"Next","tagName":"h2"},{"title":"flowtide_link​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_link","content":" Scope: OperatorType: Gauge  Describes an output flow from one operator to another, each output in an operator has its own metric series. Same as in metadata, the value is not used, and this is only described by its labels.  Label Name\tExample\tDescriptionsource\t1\tId of the operator that the link originates from (same as 'operator' label) target\t2\tId of the operator data flows to. id\t1-2\tUnique id of a link, to more easily identify new links if required.  ","version":"Next","tagName":"h2"},{"title":"flowtide_memory_allocated_bytes​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_memory_allocated_bytes","content":" Scope: OperatorType: Counter  Contains a value on how much unmanaged memory an operator has allocated. This value will never decrease, to calculate the current usage it must be subtracted by flowtide_memory_freed_bytes.  ","version":"Next","tagName":"h2"},{"title":"flowtide_memory_freed_bytes​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_memory_freed_bytes","content":" Scope: OperatorType: Counter  Contains a value on how much unmanaged memory an operator has freed. The value will never decrease. Is usually used together with flowtide_memory_allocated_bytes to calculate current memory usage.  ","version":"Next","tagName":"h2"},{"title":"flowtide_memory_allocation_count​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_memory_allocation_count","content":" Scope: OperatorType: Counter  A counter that contains how many allocation operations has been done by an operator. Can be used together withflowtide_memory_allocated_bytes to calculate average size allocations as an example.  ","version":"Next","tagName":"h2"},{"title":"flowtide_memory_free_count​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_memory_free_count","content":" Scope: OperatorType: Counter  A counter that contains how many free operations has been done by an operator. Can be used together withflowtide_memory_allocation_count to get how many current allocations exist for an operator.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_cache_tries​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_cache_tries","content":" Scope: StreamType: Counter  A counter that contains how many get operations have occured against the LRU cache. Used with flowtide_lru_table_cache_hitsand flowtide_lru_table_cache_misses to calculate hit and miss percentages.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_cache_misses​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_cache_misses","content":" Scope: StreamType: Counter  Contains a value on how many cache misses have occured against the LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_cache_hits​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_cache_hits","content":" Scope: StreamType: Counter  Contains a value on how many cache hits have occured against the LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_max_size​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_max_size","content":" Scope: StreamType: Gauge  Contains the value of the maximum amount of pages that can exist in the LRU cache, if this value is reached, the stream will halt until pages have been offloaded to disk.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_size​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_size","content":" Scope: StreamType: Gauge  Contains the current amount of pages in the LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_lru_table_cleanup_start​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_lru_table_cleanup_start","content":" Scope: StreamType: Gauge  Contains the value where offloading to disk will occur. Before this value no pages will be written to disk.  ","version":"Next","tagName":"h2"},{"title":"flowtide_temporary_write_ms​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_temporary_write_ms","content":" Scope: OperatorType: Histogram  A histogram that describes how long it takes to write to temporary storage from LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_temporary_read_ms​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_temporary_read_ms","content":" Scope: OperatorType: Histogram  A histogram that describes how long it takes to read from temporary storage into LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_persistence_read_ms​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_persistence_read_ms","content":" Scope: OperatorType: Histogram  A histogram that describes how long it takes to read from persistent storage into LRU cache.  ","version":"Next","tagName":"h2"},{"title":"flowtide_latency​","type":1,"pageTitle":"General Metrics","url":"/flowtide/docs/monitoring/generalmetrics#flowtide_latency","content":" Scope: OperatorType: Histogram  Histogram that describes how long time it takes for a watermark to traverse the stream. Each egress point has a histogram per source operator that produces watermarks. This allows a developer to gain understanding how quickly events can be resolved in the stream. ","version":"Next","tagName":"h2"},{"title":"Health checks","type":0,"sectionRef":"#","url":"/flowtide/docs/monitoring/healthchecks","content":"","keywords":"","version":"Next"},{"title":"Statuses​","type":1,"pageTitle":"Health checks","url":"/flowtide/docs/monitoring/healthchecks#statuses","content":" This section describes how the different stream statuses maps to the health check status:  Stream Status\tHealth check status\tDescriptionFailing\tUnhealthy\tStream has crashed Running\tRunning\tOperational Starting\tDegraded\tStarting is only reported when going from stopped -&gt; running Stopped\tUnhealthy\tIf a stream should be stopped, remove it from health check Degraded\tDegraded\tReported if a operator is degraded, such as slow performance ","version":"Next","tagName":"h2"},{"title":"Buffer Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/buffer","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Buffer Operator","url":"/flowtide/docs/operators/buffer#metrics","content":" The Buffer Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that the operator outputs. ","version":"Next","tagName":"h2"},{"title":"Getting started","type":0,"sectionRef":"#","url":"/flowtide/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Writing the SQL code​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#writing-the-sql-code","content":" The first step is to create an execution plan, this can be be done with any substrait plan creator. But it is also possible to do it with SQL inside flowtide. This tutorial will show you how to create a plan with SQL.  var sqlText = @&quot; INSERT INTO {sqlserver database name}.{schema name}.{destinationname} SELECT t.val FROM {sqlserver database name}.{schema name}.{tablename} t LEFT JOIN {sqlserver database name}.{schema name}.{othertablename} o ON t.val = o.val WHERE t.val = 123; &quot;;   Replace all values with that are between { } with your own table names in your SQL Server.  ","version":"Next","tagName":"h2"},{"title":"Configure the stream​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#configure-the-stream","content":" Next we will add a Flowtide stream to the service collection and add our SQL text as a execution plan:  builder.Services.AddFlowtideStream(&quot;myStream) .AddSqlTextAsPlan(sqlText)   The stream name is added on all logs and metrics, so keeping it unique in your environment can help when setting up monitoring.  ","version":"Next","tagName":"h2"},{"title":"Add connectors​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#add-connectors","content":" So far we have written SQL code and started configuring the stream. But we have not yet instructed Flowtide what it should connect to. Some examples of sinks and sources are:  MS SQLKafkaElasticsearchAnd more  This is done by using the AddConnectors method:  builder.Services.AddFlowtideStream(&quot;myStream) ... .AddConnectors(connectorManager =&gt; { // Add a SQL Server database as an available source connectorManager.AddSqlServerSource(() =&gt; &quot;Server={your server};Database={your database};Trusted_Connection=True;&quot;); // Add another SQL Server database as a sink connectorManager.AddSqlServerSink(() =&gt; &quot;Server={your server};Database={your database};Trusted_Connection=True;&quot;); })   ","version":"Next","tagName":"h2"},{"title":"Configuring state storage​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#configuring-state-storage","content":" A Flowtide stream requires state storage to function. This can be stored on a file system or on a cloud storage solution.  In this tutorial we will use a local development storage. This storage gets cleared between each test run which make it good for development.  builder.Services.AddFlowtideStream(&quot;myStream) ... .AddStorage(storage =&gt; { storage.AddTemporaryDevelopmentStorage(); });   If you want to use persistent storage on the local file system, you can instead use:  .AddStorage(storage =&gt; { storage.AddFasterKVFileSystemStorage(&quot;./stateData&quot;); });   ","version":"Next","tagName":"h2"},{"title":"Adding the UI​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#adding-the-ui","content":" If you want to add the UI to visualize the progress of the stream, add the following code after &quot;var app = builder.Build();&quot;.  app.UseFlowtideUI(&quot;/stream&quot;);   ","version":"Next","tagName":"h2"},{"title":"Full example​","type":1,"pageTitle":"Getting started","url":"/flowtide/docs/intro#full-example","content":" Here is the full code example to get started:  var builder = WebApplication.CreateBuilder(args); var sqlText = @&quot; INSERT INTO {sqlserver database name}.{schema name}.{destinationname} SELECT t.val FROM {sqlserver database name}.{schema name}.{tablename} t LEFT JOIN {sqlserver database name}.{schema name}.{othertablename} o ON t.val = o.val WHERE t.val = 123; &quot;; builder.Services.AddFlowtideStream(&quot;myStream) .AddSqlTextAsPlan(sqlText) .AddConnectors(connectorManager =&gt; { // Add a SQL Server database as an available source connectorManager.AddSqlServerSource(() =&gt; &quot;Server={your server};Database={your database};Trusted_Connection=True;&quot;); // Add another SQL Server database as a sink connectorManager.AddSqlServerSink(() =&gt; &quot;Server={your server};Database={your database};Trusted_Connection=True;&quot;); }) .AddStorage(storage =&gt; { storage.AddTemporaryDevelopmentStorage(); }); var app = builder.Build(); app.UseFlowtideUI(&quot;/stream&quot;); app.Run();  ","version":"Next","tagName":"h2"},{"title":"Prometheus","type":0,"sectionRef":"#","url":"/flowtide/docs/monitoring/prometheus","content":"","keywords":"","version":"Next"},{"title":"Sample​","type":1,"pageTitle":"Prometheus","url":"/flowtide/docs/monitoring/prometheus#sample","content":" You can find a sample in github to see how it can be setup. ","version":"Next","tagName":"h2"},{"title":"Aggregate Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/aggregate","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Aggregate Operator","url":"/flowtide/docs/operators/aggregate#metrics","content":" The Aggregate Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not.  info At this point, an aggregate operator will never be unhealthy. If there is a failure against the state, the stream will instead restart. ","version":"Next","tagName":"h2"},{"title":"Filter Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/filter","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Filter Operator","url":"/flowtide/docs/operators/filter#metrics","content":" The Projection Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not.  info At this point, a filter operator will never be unhealthy. ","version":"Next","tagName":"h2"},{"title":"Normalization Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/normalization","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Normalization Operator","url":"/flowtide/docs/operators/normalization#metrics","content":" The Normalization Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that pass through the operator.  info At this point, a normalization operator will never be unhealthy. If there is a failure against the state, the stream will instead restart. ","version":"Next","tagName":"h2"},{"title":"Projection Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/projection","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Projection Operator","url":"/flowtide/docs/operators/projection#metrics","content":" The Projection Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that pass through the operator.  info At this point, a projection operator will never be unhealthy. ","version":"Next","tagName":"h2"},{"title":"Join Operators","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/join","content":"","keywords":"","version":"Next"},{"title":"Merge-Join Operator​","type":1,"pageTitle":"Join Operators","url":"/flowtide/docs/operators/join#merge-join-operator","content":" The merge-join operator is a stateful operator that is implemented by two different B+ trees, one for each input source. The trees are sorted based on the keys used in the equality condition.  ","version":"Next","tagName":"h2"},{"title":"Metrics​","type":1,"pageTitle":"Join Operators","url":"/flowtide/docs/operators/join#metrics","content":" Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that pass through the operator.  info At this point, a merge-join operator will never be unhealthy.  ","version":"Next","tagName":"h3"},{"title":"Block-Nested Join Operator​","type":1,"pageTitle":"Join Operators","url":"/flowtide/docs/operators/join#block-nested-join-operator","content":" The block-nested join operator is a stateful operator that is implemented using 2 persistent B+ trees, and two temporary B+ trees. The temporary trees fill up with data until a watermark is recieved in which they it performs the join operations. It does this to reduce the amount of I/O that has to be made when reading through the entire persisted dataset.  ","version":"Next","tagName":"h2"},{"title":"Metrics​","type":1,"pageTitle":"Join Operators","url":"/flowtide/docs/operators/join#metrics-1","content":" Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not.  info At this point, a block-nested join operator will never be unhealthy. If there is a failure against the state, the stream will instead restart. ","version":"Next","tagName":"h3"},{"title":"Set Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/set","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Set Operator","url":"/flowtide/docs/operators/set#metrics","content":" The Set Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that the operator outputs. events_processed\tCounter\tHow many events that the operator processes.  info At this point, a set operator will never be unhealthy. If there is a failure against the state, the stream will instead restart. ","version":"Next","tagName":"h2"},{"title":"Table Function Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/tablefunction","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Table Function Operator","url":"/flowtide/docs/operators/tablefunction#metrics","content":" The Table Function Operators have the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events that the operator outputs. events_processed\tCounter\tHow many events the operator processes.  info At this point, a table function operator will never be unhealthy. ","version":"Next","tagName":"h2"},{"title":"Iteration Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/iteration","content":"","keywords":"","version":"Next"},{"title":"Iteration Relation​","type":1,"pageTitle":"Iteration Operator","url":"/flowtide/docs/operators/iteration#iteration-relation","content":" The iteration relation uses ExtensionMultiRel in substrait and is defined as follows in protobuf:  message IterationRelation { string iterationName = 1; }   The first input in ExtensionMultiRel is the loop relation, the second input is input to the iteration itself. The second input is optional.  IterationName exist since there can be multiple nested iterations.  ","version":"Next","tagName":"h2"},{"title":"Iteration Reference Read Relation​","type":1,"pageTitle":"Iteration Operator","url":"/flowtide/docs/operators/iteration#iteration-reference-read-relation","content":" This relation is used to tell where the data from the iteration operator should be sent to inside of the loop. This relation should only be used inside the loop relation.  It is defined by ExtensionLeafRel and has the following message:  message IterationReferenceReadRelation { // Name of the iteration to get data from string iterationName = 1; }   ","version":"Next","tagName":"h2"},{"title":"Implementation​","type":1,"pageTitle":"Iteration Operator","url":"/flowtide/docs/operators/iteration#implementation","content":" The iteration operator differs a bit from the other operators in how it does checkpointing. To make sure a checkpoint contains all processed data before comitting to a checkpoint it follows these steps:  If there is no input to the operator, a dummy read operator is created that only sends checkpoint events.On checkpoint send a LockingEventPrepare to the loop.All operators in loop adds information if they have another dependency that is not yet in checkpoint.If any message was recieved before the iteration operator recieves the LockingEventPrepare message, or a dependency is not in checkpoint, the message is resent.When all conditions above are met, the checkpoint is sent throught the loop.When the operator recieves the checkpoint from the loop, it first sends out watermark information, and then the checkpoint to the rest of the stream.  ","version":"Next","tagName":"h2"},{"title":"Metrics​","type":1,"pageTitle":"Iteration Operator","url":"/flowtide/docs/operators/iteration#metrics","content":" The Iteration Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not.  info At this point, an iteration operator will never be unhealthy. If there is a failure against the state, the stream will instead restart. ","version":"Next","tagName":"h2"},{"title":"Top N Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/topn","content":"","keywords":"","version":"Next"},{"title":"Metrics​","type":1,"pageTitle":"Top N Operator","url":"/flowtide/docs/operators/topn#metrics","content":" The Top N Operator has the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events the operator outputs. ","version":"Next","tagName":"h2"},{"title":"Window Operator","type":0,"sectionRef":"#","url":"/flowtide/docs/operators/window","content":"","keywords":"","version":"Next"},{"title":"Behavior​","type":1,"pageTitle":"Window Operator","url":"/flowtide/docs/operators/window#behavior","content":" When events are received:  Rows are inserted into the Persistent Tree.If a row is deleted (i.e., an event with negative weight), that row's negative output is sent downstream immediately.The partition key is calculated and recorded in the Temporary Partition Tree.  For upsert operations, the row is held until a watermark is received, at which point the final calculation is performed.  When a watermark event is received, the operator:  Iterates over all changed partitions.Applies the configured window function.  The logic for how values are calculated within a partition is delegated to the specific window function implementation.  ","version":"Next","tagName":"h2"},{"title":"Metrics​","type":1,"pageTitle":"Window Operator","url":"/flowtide/docs/operators/window#metrics","content":" The Window Operator exposes the following metrics:  Metric Name\tType\tDescriptionbusy\tGauge\tValue 0-1 on how busy the operator is. backpressure\tGauge\tValue 0-1 on how much backpressure the operator has. health\tGauge\tValue 0 or 1, if the operator is healthy or not. events\tCounter\tHow many events the operator outputs. events_processed\tCounter\tHow many events the operator recieves. ","version":"Next","tagName":"h2"},{"title":"SQL","type":0,"sectionRef":"#","url":"/flowtide/docs/sql","content":"SQL Flowtide has SQL support which will transform the SQL into a substrait plan which can then be run the engine. To create a plan from SQL, add the following code to your application: var sqlBuilder = new SqlPlanBuilder(); sqlBuilder.Sql(@&quot; CREATE TABLE testtable ( val any ); INSERT INTO output SELECT t.val FROM testtable t &quot;); var plan = sqlBuilder.GetPlan(); info All SQL plans must have a source and a sink, so it must always insert to somewhere. The INSERT INTO denotes which output should leave the stream. You can find more information in the following chapters: 📄️ Table Provider It is possible to add table providers to the SqlPlanBuilder. These providers are called each time the compiler finds the usage of a table. 📄️ Create Table The CREATE TABLE data definition is used to add metadata about a table to the SqlPlanBuilder. 📄️ Create View The create view command allows you to define a reusable sub-plan. 🗃️ Select 4 items 📄️ Insert Into The INSERT INTO statement is used to send data from the stream into a sink. Each stream requires at least one insert into statement to mark which data should be output from the stream. 📄️ Recursion Flowtide also supports recusive queries, such as iterating over a tree structure. This is done with the WITH statement, and works similar as in other databases.","keywords":"","version":"Next"},{"title":"Create Table","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/createtable","content":"Create Table The CREATE TABLE data definition is used to add metadata about a table to the SqlPlanBuilder. Since in SQL, multiple tables might have the same column name, the builder must know what columns exist in which table. Since Flowtide is typeless when defining inputs, it is not required to say a datatype when using create table. Example usage: CREATE TABLE table_name ( column1, column2, column3, .... ); Creating table definitions is not required if you are using table providers.","keywords":"","version":"Next"},{"title":"Parallelism","type":0,"sectionRef":"#","url":"/flowtide/docs/parallelism","content":"Parallelism warning Parallelism support is still experimental. This setting can not yet be applied when creating a stream using Dependency Injection setup. Some operators support running in parallel. This is done by partitioning the input into an operator. For instance in an aggregate with a grouping. It can partition the data on the grouping values. This can help reduce bottlenecks on certain operators that do complex operations. To run a stream with parallelism enabled, add the following to the FlowtideBuilder: flowtideBuilder // Set an integer number here .SetParallelism(parallelism) ","keywords":"","version":"Next"},{"title":"Create View","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/createview","content":"","keywords":"","version":"Next"},{"title":"Buffered view​","type":1,"pageTitle":"Create View","url":"/flowtide/docs/sql/createview#buffered-view","content":" It is possible to create a buffered view. It is a view that collects all the output from the view in to temporary storage and waits for a watermark.  This can reduce the output from the view in situations where data is updated regularly but gives the same output value. One example is when a column value is based on gettimestamp such as gettimestamp() &gt; date which will give the same output the majority of the time. The buffered view will then only give out the changed rows, and can reduce computational load in the result of the stream.  It adds a buffer operator at the end of the view.  Example:  CREATE VIEW buffered WITH (BUFFERED = true) AS SELECT CASE WHEN orderdate &lt; gettimestamp() THEN true ELSE false END as active FROM orders;  ","version":"Next","tagName":"h2"},{"title":"Insert Into","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/insertinto","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Insert Into","url":"/flowtide/docs/sql/insertinto#example","content":" INSERT INTO outputtable SELECT column1, column2 FROM inputtable  ","version":"Next","tagName":"h2"},{"title":"Except","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/select/except","content":"","keywords":"","version":"Next"},{"title":"Except All​","type":1,"pageTitle":"Except","url":"/flowtide/docs/sql/select/except#except-all","content":" The EXCEPT ALL operator returns rows that exist in the first statement, which does not exist in any other rows from the other statements.  select_stament1 EXCEPT ALL select_stament2   ","version":"Next","tagName":"h2"},{"title":"Except Distinct​","type":1,"pageTitle":"Except","url":"/flowtide/docs/sql/select/except#except-distinct","content":" The EXCEPT DISTINCT operator returns distinct rows that exist in the first statement, which does not exist in any other rows from the other statements.  select_stament EXCEPT DISTINCT select_stament  ","version":"Next","tagName":"h2"},{"title":"Select","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/select","content":"","keywords":"","version":"Next"},{"title":"Wildcards​","type":1,"pageTitle":"Select","url":"/flowtide/docs/sql/select#wildcards","content":" Wildcards can be used in SQL projections, as demonstrated in the example below:  SELECT * FROM table1   However, using wildcards is generally discouraged. They can lead to the inclusion of unnecessary columns and may result in errors if the structure of the source table changes. ","version":"Next","tagName":"h2"},{"title":"Recursion","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/recursion","content":"Recursion Flowtide also supports recusive queries, such as iterating over a tree structure. This is done with the WITH statement, and works similar as in other databases. Example: with user_manager_cte AS ( SELECT userKey, firstName, lastName, managerKey, null as ManagerFirstName, 1 as level FROM users WHERE managerKey is null UNION ALL SELECT u.userKey, u.firstName, u.lastName, u.managerKey, umc.firstName as ManagerFirstName, level + 1 as level FROM users u INNER JOIN user_manager_cte umc ON umc.userKey = u.managerKey ) INSERT INTO output SELECT userKey, firstName, lastName, managerKey, ManagerFirstName, level FROM user_manager_cte ","keywords":"","version":"Next"},{"title":"Top N","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/select/topn","content":"Top N The Top N operator returns only the top N results from a query. An ordering should be provided as well. Example: SELECT TOP (10) userkey FROM users ORDER BY userkey ","keywords":"","version":"Next"},{"title":"Union","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/select/union","content":"","keywords":"","version":"Next"},{"title":"Union All​","type":1,"pageTitle":"Union","url":"/flowtide/docs/sql/select/union#union-all","content":" The UNION ALL operator concatenates results from one or several SELECT statements.  select_stament UNION ALL select_stament   ","version":"Next","tagName":"h2"},{"title":"Union Distinct​","type":1,"pageTitle":"Union","url":"/flowtide/docs/sql/select/union#union-distinct","content":" The UNION DISTINCT operator concatenates results from one or several SELECT statements and only return the distinct rows.  select_stament UNION DISTINCT select_stament  ","version":"Next","tagName":"h2"},{"title":"Intersect","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/select/intersect","content":"","keywords":"","version":"Next"},{"title":"Intersect All​","type":1,"pageTitle":"Intersect","url":"/flowtide/docs/sql/select/intersect#intersect-all","content":" The INTERSECT ALL operator returns rows that exist in all statements, this includes duplicates.  select_stament1 INTERSECT ALL select_stament2   ","version":"Next","tagName":"h2"},{"title":"Intersect Distinct​","type":1,"pageTitle":"Intersect","url":"/flowtide/docs/sql/select/intersect#intersect-distinct","content":" The INTERSECT DISTINCT operator returns rows that exist in all statements, this outputs distinct rows.  select_stament INTERSECT DISTINCT select_stament  ","version":"Next","tagName":"h2"},{"title":"Table Provider","type":0,"sectionRef":"#","url":"/flowtide/docs/sql/tableprovider","content":"","keywords":"","version":"Next"},{"title":"Registering a table provider​","type":1,"pageTitle":"Table Provider","url":"/flowtide/docs/sql/tableprovider#registering-a-table-provider","content":" You register a table provider on the SqlPlanBuilder by calling the AddTableProvider method:  var sqlBuilder = new SqlPlanBuilder(); sqlBuilder.AddTableProvider(new MyCustomProvider());   Some connectors, such as SQL Server connector might already come with a provider, a connector usually add an extension method to add its provider. Example on how SQL Server connector provider is added:  sqlBuilder.AddSqlServerProvider(() =&gt; connectionString);  ","version":"Next","tagName":"h2"},{"title":"Testing","type":0,"sectionRef":"#","url":"/flowtide/docs/testing","content":"Testing This section covers how one can run tests for a stream. A specific nuget exists to help with testing: FlowtideDotNet.TestFramework 📄️ Integration Testing This section assumes that the Flowtide stream is implemented as a webapplication with minimal API. 📄️ .NET Aspire Testing If you run your stream with the help of .NET Aspire it may be beneficial to run the tests using the Aspire test framework.","keywords":"","version":"Next"},{"title":".NET Aspire Testing","type":0,"sectionRef":"#","url":"/flowtide/docs/testing/aspiretesting","content":"","keywords":"","version":"Next"},{"title":"Add the test information HTTP endpoint​","type":1,"pageTitle":".NET Aspire Testing","url":"/flowtide/docs/testing/aspiretesting#add-the-test-information-http-endpoint","content":" To be able to get information about the stream status such as checkpoint information, you must map a helper endpoint in your stream application. This is done by calling MapFlowtideTestInformation.  Example:  if (builder.Configuration.GetValue&lt;bool&gt;(&quot;TEST_MODE&quot;)) { // If we are in test mode, map the test endpoint app.MapFlowtideTestInformation(); }   ","version":"Next","tagName":"h2"},{"title":"Enable the test mode configuration in the Aspire configuration​","type":1,"pageTitle":".NET Aspire Testing","url":"/flowtide/docs/testing/aspiretesting#enable-the-test-mode-configuration-in-the-aspire-configuration","content":" The next step is to add in .NET Aspire configuration that the test mode configuration is enabled:  var project = builder.AddProject&lt;MyStreamProject&gt;(&quot;stream&quot;); ... if (builder.Configuration.GetValue&lt;bool&gt;(&quot;test_mode&quot;)) { project = project.WithEnvironment(&quot;TEST_MODE&quot;, &quot;true&quot;); }   This enables turning on test mode from the integration tests.  ","version":"Next","tagName":"h2"},{"title":"Wait for checkpoints in the integration test​","type":1,"pageTitle":".NET Aspire Testing","url":"/flowtide/docs/testing/aspiretesting#wait-for-checkpoints-in-the-integration-test","content":" The final step is to start checking for checkpoints in the integration tests. Since a checkpoint happens after a data change and is considered a stable state, it is best to run assertions first after a checkpoint has happened.  [Fact] public async Task MyTest() { var appHost = await DistributedApplicationTestingBuilder.CreateAsync&lt;Projects.MyAspire&gt;([ &quot;test_mode=true&quot; ]); await using var app = await appHost.BuildAsync(); var resourceNotificationService = app.Services.GetRequiredService&lt;ResourceNotificationService&gt;(); await app.StartAsync(); var httpClient = app.CreateHttpClient(&quot;my_project_resource_name&quot;); await resourceNotificationService.WaitForResourceAsync(&quot;my_project_resource_name&quot;, KnownResourceStates.Running).WaitAsync(TimeSpan.FromSeconds(30)); // Create the stream http monitor var streamMonitor = new StreamTestHttpMonitor(httpClient, &quot;my_stream_name&quot;); // Wait for a checkpoint to occur await streamMonitor.WaitForCheckpoint(); // Assert here against your destination }   If you want to test modifying data while the stream is running, it may be beneficial to pause and resume it. To do that, also add UseFlowtidePauseResumeEndpoints to the exposed endpoints when testing. Please read Pause And Resume for more information. ","version":"Next","tagName":"h2"},{"title":"State Persistence","type":0,"sectionRef":"#","url":"/flowtide/docs/statepersistence","content":"","keywords":"","version":"Next"},{"title":"FasterKV storage​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#fasterkv-storage","content":" FasterKV is persistent key value store built by Microsoft. It is the only storage solution available for Flowtide that will persist data between runs. FasterKV is highly configurable, and how you configure it will affect the performance of your stream.  To configure your stream to use FasterKV storage, add the following to the builder:  builder .WithStateOptions(() =&gt; new StateManagerOptions() { PersistentStorage = new FasterKvPersistentStorage(new FasterKVSettings&lt;long, SpanByte&gt;() { // Set the fasterKV configuration here ... }) });   ","version":"Next","tagName":"h2"},{"title":"Useful configuration options​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#useful-configuration-options","content":" Property\tDescriptionLogDevice\tThe log device that will write to storage MemorySize\tHow much memory FasterKV can use PageSize\thow large a page is CheckpointDir\tWhere checkpoints should be stored CheckpointManager\tCheckpoint manager, useful if using Azure Storage.  ","version":"Next","tagName":"h3"},{"title":"Storing to disk​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#storing-to-disk","content":" This is an example of a configuration to store to a disk.  var baseDirectory = &quot;/persistence/&quot; builder.WithStateOptions(() =&gt; new StateManagerOptions() { // Set cache page count to reduce the memory usage CachePageCount = 10000, PersistentStorage = new FasterKvPersistentStorage(new FasterKVSettings&lt;long, SpanByte&gt;() { // Checkpoint directory CheckpointDir = $&quot;{baseDirectory}/checkpoints&quot;, // A local file log device LogDevice = Devices.CreateLogDevice($&quot;{baseDirectory}/log&quot;), // Redice memory usage of fasterKV, to limit memory usage MemorySize = 1024L * 1024L * 64, // Page size PageSize = 1024 * 1024 * 16, }), TemporaryStorageOptions = new FileCacheOptions() { // Path where the temporary cache is stored DirectoryPath = $&quot;./temp&quot; } })   ","version":"Next","tagName":"h3"},{"title":"Storing to Azure Storage​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#storing-to-azure-storage","content":" Storing the data in an Azure Storage requires a bit more configuration, especially a checkpoint manager.  // Create azure storage device var log = new AzureStorageDevice(STORAGE_STRING, BASE_CONTAINER, &quot;&quot;, &quot;hlog.log&quot;); // Create azure storage backed checkpoint manager var checkpointManager = new DeviceLogCommitCheckpointManager( new AzureStorageNamedDeviceFactory(STORAGE_STRING), new DefaultCheckpointNamingScheme($&quot;{BASE_CONTAINER}/checkpoints/&quot;)); builder.WithStateOptions(() =&gt; new StateManagerOptions() { // Set cache page count to reduce the memory usage CachePageCount = 10000, PersistentStorage = new FasterKvPersistentStorage(new FasterKVSettings&lt;long, SpanByte&gt;() { CheckpointManager = checkpointManager, LogDevice = log, // Redice memory usage of fasterKV, to limit memory usage MemorySize = 1024L * 1024L * 64, // Page size PageSize = 1024 * 1024 * 16, }), TemporaryStorageOptions = new FileCacheOptions() { // Path where the temporary cache is stored DirectoryPath = $&quot;./temp&quot; } })   ","version":"Next","tagName":"h3"},{"title":"Temporary file cache storage​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#temporary-file-cache-storage","content":" This storage solution is useful when developing or running unit tests on a stream. All data will be cleared between each run, but it will be persisted to local disk to reduce RAM usage and allow you to run streams with alot of data.  The implementation of this is using the same solution as the intermediate file cache solution where modified pages are stored between checkpoints.  To configure your stream to use this storage solution, add the following to the stream builder:  builder .WithStateOptions(() =&gt; new StateManagerOptions() { // This is non persistent storage, use FasterKV persistence storage instead if you want persistent storage PersistentStorage = new FileCachePersistentStorage(new FlowtideDotNet.Storage.FileCacheOptions() { DirectoryPath = &quot;./tmp&quot; }) });   ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#configuration","content":" Property\tDefault value\tDescriptionDirectoryPath\t./data/tempFiles\tPath where the files will be stored  ","version":"Next","tagName":"h3"},{"title":"SQL server storage​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#sql-server-storage","content":" warning SQL Server storage support is still experimental.  Store persistent data to sql server.  Before using this storage solution you must manually create required tables using this creation script: Sql tables creation script.  The sql user running the system requires the following specific permissions:  SELECTINSERTDELETEUPDATE  builder.Services.AddFlowtideStream(&quot;yourstream&quot;) [...] .AddStorage(s =&gt; { // register sql server storage using default settings s.AddSqlServerStorage(&quot;[connectionstring]&quot;); // register sql server storge using function to retrieve the connection string. s.AddSqlServerStorage(() =&gt; &quot;[connectionstring]&quot;); // or use the overload to specify more settings s.AddSqlServerStorage(new SqlServerPersistentStorageSettings() { ConnectionStringFunc = () =&gt; builder.Configuration.GetConnectionString(&quot;[connectionstring]&quot;), // if you created the tables on a non default schema (or with another name) you can specify the full name for the tables used here. // it's also possible to specify the database name as part of table name. StreamTableName = &quot;[MySchema].[Streams]&quot;, StreamPageTableName = &quot;[MyDatabase].[MySchema].[StreamPages2]&quot; }); });   ","version":"Next","tagName":"h2"},{"title":"Storage solution​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#storage-solution","content":" The stream storage is built on a three tier architecture, there is the in memory cache, the local disk modified page cache, and the persistent data.  A data page is fetched using the following logic:    ","version":"Next","tagName":"h2"},{"title":"Compression​","type":1,"pageTitle":"State Persistence","url":"/flowtide/docs/statepersistence#compression","content":" It is possible to compress pages in the state. The option that exist today is to compress pages with Zstd. Most storage backends add zstd compression by default to save on network throughput and storage size.  To set compression, it is set under add storage:  builder.AddStorage(b =&gt; { ... // Use zstd page compression b.ZstdPageCompression(); // Use no compression even if the storage medium added compression b.NoCompression(); });  ","version":"Next","tagName":"h2"},{"title":"Integration Testing","type":0,"sectionRef":"#","url":"/flowtide/docs/testing/integrationtests","content":"","keywords":"","version":"Next"},{"title":"Configure WebApplicationFactory​","type":1,"pageTitle":"Integration Testing","url":"/flowtide/docs/testing/integrationtests#configure-webapplicationfactory","content":" Create a class for your integration tests and configure a WebApplicationFactory.  With the WebApplicationFactory it is possible to override connectors and storage to allow using different connectors for the test.  public class IntegrationTests : IDisposable { private readonly WebApplicationFactory&lt;Program&gt; _factory; private readonly StreamTestMonitor _inProcessMonitor; public IntegrationTests() { _factory = new WebApplicationFactory&lt;Program&gt;().WithWebHostBuilder(b =&gt; { b.ConfigureTestServices(services =&gt; { services.AddFlowtideStream(&quot;stream&quot;) // Add to override connectors .AddConnectors(c =&gt; { // Add new connectors here }) .AddStorage(storage =&gt; { // Change to temporary storage for testing storage.AddTemporaryDevelopmentStorage(); }) // Add a test monitor that can be used to check for checkpoints (which ensures data updates) .AddStreamTestMonitor(_inProcessMonitor); }); }); } public void Dispose() { _factory.Dispose(); } }   ","version":"Next","tagName":"h2"},{"title":"Add a test table as a source​","type":1,"pageTitle":"Integration Testing","url":"/flowtide/docs/testing/integrationtests#add-a-test-table-as-a-source","content":" It is possible to create a test table where you can create mock data used for the tests.  Example:  public class IntegrationTests : IDisposable { private TestDataTable _source; ... public IntegrationTests() { _source = TestDataTable.Create(new[] { new { val = 0 }, new { val = 1 }, new { val = 2 }, new { val = 3 }, new { val = 4 } }); _factory = new WebApplicationFactory&lt;Program&gt;().WithWebHostBuilder(b =&gt; { b.ConfigureTestServices(services =&gt; { services.AddFlowtideStream(&quot;stream&quot;) // Add to override connectors .AddConnectors(c =&gt; { c.AddTestDataTable(&quot;testtable&quot;, _source); }) ... }); }); } }   A test data table can be added under AddConnectors with the specific table name it should be registered under.  ","version":"Next","tagName":"h2"},{"title":"Add a test data sink​","type":1,"pageTitle":"Integration Testing","url":"/flowtide/docs/testing/integrationtests#add-a-test-data-sink","content":" It may be useful to add a test data sink which allows evaluating the output of the stream. This can be used to evaluate that the query plan actually results with the expected data.  public class IntegrationTests : IDisposable { private TestDataSink _sink; ... public IntegrationTests() { _sink = new TestDataSink(); _factory = new WebApplicationFactory&lt;Program&gt;().WithWebHostBuilder(b =&gt; { b.ConfigureTestServices(services =&gt; { services.AddFlowtideStream(&quot;stream&quot;) // Add to override connectors .AddConnectors(c =&gt; { ... // Test data sink is added using a regexp expression that matches destination names c.AddTestDataSink(&quot;.*&quot;, _sink); }) ... }); }); } }   ","version":"Next","tagName":"h2"},{"title":"Creating a test​","type":1,"pageTitle":"Integration Testing","url":"/flowtide/docs/testing/integrationtests#creating-a-test","content":" The next step is to create the actual test case to test the stream.  [Fact] public async Task TestStreamOutput() { _factory.CreateClient(); //Create a client to start the stream // Wait for the stream to checkpoint before asserting the resulting data await _inProcessMonitor.WaitForCheckpoint(); Assert.True(_sink.IsCurrentDataEqual(new[] { new { val = 0 }, new { val = 1 }, new { val = 2 }, new { val = 3 }, new { val = 4 } })); }   ","version":"Next","tagName":"h2"},{"title":"Full example​","type":1,"pageTitle":"Integration Testing","url":"/flowtide/docs/testing/integrationtests#full-example","content":" Here is the full example of the test class:  public class IntegrationTests : IDisposable { private readonly WebApplicationFactory&lt;Program&gt; _factory; private TestDataSink _sink; private TestDataTable _source; private StreamTestMonitor _inProcessMonitor; public IntegrationTests() { _source = TestDataTable.Create(new[] { new { val = 0 }, new { val = 1 }, new { val = 2 }, new { val = 3 }, new { val = 4 } }); _sink = new TestDataSink(); _inProcessMonitor = new StreamTestMonitor(); _factory = new WebApplicationFactory&lt;Program&gt;().WithWebHostBuilder(b =&gt; { b.ConfigureTestServices(services =&gt; { services.AddFlowtideStream(&quot;stream&quot;) .AddConnectors(c =&gt; { // Override connectors c.AddTestDataTable(&quot;testtable&quot;, _source); c.AddTestDataSink(&quot;.*&quot;, _sink); }) .AddStorage(storage =&gt; { // Change to temporary storage for unit tests storage.AddTemporaryDevelopmentStorage(); }) .AddStreamTestMonitor(_inProcessMonitor); }); }); } public void Dispose() { _factory.Dispose(); } [Fact] public async Task TestStreamOutput() { _factory.CreateClient(); //Create a client to start the stream await _inProcessMonitor.WaitForCheckpoint(); Assert.True(_sink.IsCurrentDataEqual(new[] { new { val = 0 }, new { val = 1 }, new { val = 2 }, new { val = 3 }, new { val = 4 } })); // Add a new row _source.AddRows(new { val = 5 }); // Remove a row _source.RemoveRows(new { val = 3 }); await _inProcessMonitor.WaitForCheckpoint(); Assert.True(_sink.IsCurrentDataEqual(new[] { new { val = 0 }, new { val = 1 }, new { val = 2 }, new { val = 4 }, new { val = 5 } })); } }  ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}