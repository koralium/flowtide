"use strict";(self.webpackChunkflowtide=self.webpackChunkflowtide||[]).push([[6698],{452:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"connectors/kafka","title":"Kafka Connector","description":"Source","source":"@site/docs/connectors/kafka.md","sourceDirName":"connectors","slug":"/connectors/kafka","permalink":"/flowtide/docs/connectors/kafka","draft":false,"unlisted":false,"editUrl":"https://github.com/koralium/flowtide/tree/main/docs/docs/connectors/kafka.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Elasticsearch Connector","permalink":"/flowtide/docs/connectors/elasticsearch"},"next":{"title":"Custom Data Source","permalink":"/flowtide/docs/connectors/customdata"}}');var i=t(4848),a=t(8453);const r={sidebar_position:5},l="Kafka Connector",o={},c=[{value:"Source",id:"source",level:2},{value:"Usage in SQL",id:"usage-in-sql",level:3},{value:"Sink",id:"sink",level:2},{value:"Compare against existing data in Kafka",id:"compare-against-existing-data-in-kafka",level:3},{value:"Extend event processing logic",id:"extend-event-processing-logic",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"kafka-connector",children:"Kafka Connector"})}),"\n",(0,i.jsx)(n.h2,{id:"source",children:"Source"}),"\n",(0,i.jsx)(n.p,{children:"The Kafka Source allows a stream to fetch data from a kafka stream as a table in a stream."}),"\n",(0,i.jsxs)(n.p,{children:["The source is added to the ",(0,i.jsx)(n.em,{children:"ConnectorManager"})," with the following line:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:'connectorManager.AddKafkaSource("your regexp on table names",  new FlowtideKafkaSourceOptions()\n    {\n        ConsumerConfig = kafkaConsumerConfig,\n        KeyDeserializer = keyDeserializer,\n        ValueDeserializer = valueDeserializer\n    });\n'})}),"\n",(0,i.jsx)(n.p,{children:"The table name in the read relation becomes the topic name the source will read from."}),"\n",(0,i.jsx)(n.p,{children:"The following key deserializers exist:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtideKafkaJsonKeyDeserializer"})," - deserializes a json key"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtidekafkaStringKeyDeserializer"})," - deserializes a string key"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The following value deserializers exist:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtideKafkaUpsertJsonDeserializer"})," - Deserializes the value as json, if the value is not null, its an upsert, if it is null, its a delete."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The source has a special column name for the key, it is called ",(0,i.jsx)(n.em,{children:"_key"})," which can be used to select the key value from a kafka message."]}),"\n",(0,i.jsx)(n.h3,{id:"usage-in-sql",children:"Usage in SQL"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE my_kafka_topic (\n    _key,\n    firstName,\n    lastName\n);\n\nINSERT INTO outputtable\nSELECT _key, firstName, lastName\nFROM my_kafka_topic;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"sink",children:"Sink"}),"\n",(0,i.jsx)(n.p,{children:"The kafka sink allows a stream to write events to kafka."}),"\n",(0,i.jsxs)(n.p,{children:["The sink is added to the ",(0,i.jsx)(n.em,{children:"ConnectorManager"})," with the following line:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:'connectorManager.AddKafkaSink("your regexp on table names",  new FlowtideKafkaSinkOptions()\n    {\n        KeySerializer = new FlowtideKafkaStringKeySerializer(),\n        ProducerConfig = config,\n        ValueSerializer = new FlowtideKafkaUpsertJsonSerializer()\n    });\n'})}),"\n",(0,i.jsx)(n.p,{children:"Same as the source, it writes to the topic name that is entered in the write relation."}),"\n",(0,i.jsx)(n.p,{children:"Available key serializers:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtideKafkaJsonKeySerializer"})," - JSON serializes the key value"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtideKafkaStringKeySerializer"})," - Outputs a string value, only works if the key is in a string type."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Available value serializers:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FlowtideKafkaUpsertJsonSerializer"})," - Outputs the value as json, if it is a delete, it outputs null as the value."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"compare-against-existing-data-in-kafka",children:"Compare against existing data in Kafka"}),"\n",(0,i.jsx)(n.p,{children:"It is possible to fetch all existing data in Kafka when a new stream is starting. This data will be used on the initial result set\nto compare against to see if the data is already in kafka (the lateset message on a key)."}),"\n",(0,i.jsx)(n.p,{children:"It will also send delete operations for any keys that no longer exist in the result set."}),"\n",(0,i.jsx)(n.p,{children:"To use this feature you must set the following properties:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FetchExistingConfig"})," - Consumer config that will be used when fetching existing data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FetchExistingValueDeserializer"})," - Deserializer that will be used, when deserializing existing data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FetchExistingKeyDeserializer"})," - Deserializer for the key field in a kafka message."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"extend-event-processing-logic",children:"Extend event processing logic"}),"\n",(0,i.jsx)(n.p,{children:"There are two properties in the options that can help add extra logic to the kafka sink."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EventProcessor"})," - Called on all events that will be sent to kafka."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OnInitialDataSent"})," - Called once on a new stream when the first data has been sent (usually after the first checkpoint)."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These functions can help implement logic such as having an external state store for a kafka topic that keeps track of all the data\nthat has been sent to the topic. If a stream is modified and republished and has to start from the beginning, this state store\ncan then make sure only delta valeus are sent to the kafka topic, and deletions of events that no longer exists."}),"\n",(0,i.jsx)(n.p,{children:"Example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:'connectorManager.AddKafkaSink("your regexp on table names",  new FlowtideKafkaSinkOptions()\n    {\n        ...\n        EventProcessor = async (events) => {\n            for (int i = 0; i < events.Count; i++) {\n                // Check if the event exists with the exact key and value in the store\n                var exists = await dbLookup.ExistAsync(events[i].Key, events[i].Value);\n                if (exists) {\n                    events.RemoveAt(i);\n                    i--;\n                }\n                // Add the data with a run version that should be unique of this stream version.\n                await dbLookup.Add(events[i].Key, events[i].Value, runVersion);\n            }\n        },\n        OnInitialDataSent = (producer, writeRelation, topicName) => {\n            // Get all events that does not have the latest run version\n            await foreach (var e in dbLookup.GetEventsNotMatchingVersion(runVersion)) {\n                // Send delete on all events that no longer exist\n                await producer.SendAsync(new Message<byte[], byte[]?>() {\n                    Key = e.Key,\n                    Value = null\n                });\n            }\n            // Delete the events from the state store. \n            await dbLookup.DeleteNotMatchingVersion(runVersion);\n        }\n    });\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);